{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[[  2.84691714e-05   2.57176577e-03   9.97399765e-01]]\n",
      "sum: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n",
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import  (StandardScaler,MinMaxScaler)\n",
    "from sklearn.model_selection import ( KFold , StratifiedKFold, cross_val_score, GridSearchCV) \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X,Y = load_iris().data, load_iris().target\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X, Y)\n",
    "\n",
    "print( mlp.predict([3.1,  2.5,  8.4,  2.2]))\n",
    "print( mlp.predict_proba([3.1,  2.5,  8.4,  2.2]) )\n",
    "print (\"sum: %f\"%np.sum(mlp.predict_proba([3.1,  2.5,  8.4,  2.2])) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12378840\n",
      "Iteration 1, loss = 1.12343189\n",
      "Iteration 1, loss = 1.12345772\n",
      "Iteration 1, loss = 1.12148218\n",
      "Iteration 2, loss = 1.10562332\n",
      "Iteration 2, loss = 1.10492836\n",
      "Iteration 2, loss = 1.10336039\n",
      "Iteration 2, loss = 1.10549178\n",
      "Iteration 3, loss = 1.08526602\n",
      "Iteration 3, loss = 1.08289611\n",
      "Iteration 3, loss = 1.08407727\n",
      "Iteration 3, loss = 1.08522447\n",
      "Iteration 4, loss = 1.06507308\n",
      "Iteration 4, loss = 1.06343222\n",
      "Iteration 4, loss = 1.06255837\n",
      "Iteration 4, loss = 1.06501768\n",
      "Iteration 5, loss = 1.04247530\n",
      "Iteration 5, loss = 1.04316581\n",
      "Iteration 5, loss = 1.04536016\n",
      "Iteration 5, loss = 1.04517257\n",
      "Iteration 6, loss = 1.02132325\n",
      "Iteration 6, loss = 1.02068777\n",
      "Iteration 6, loss = 1.02423631\n",
      "Iteration 7, loss = 0.99719337\n",
      "Iteration 6, loss = 1.02384721\n",
      "Iteration 7, loss = 0.99659341\n",
      "Iteration 7, loss = 1.00062885\n",
      "Iteration 8, loss = 0.97016620\n",
      "Iteration 7, loss = 1.00028140\n",
      "Iteration 8, loss = 0.96960169\n",
      "Iteration 8, loss = 0.97419562\n",
      "Iteration 9, loss = 0.93934105\n",
      "Iteration 8, loss = 0.97384343\n",
      "Iteration 9, loss = 0.93883724\n",
      "Iteration 9, loss = 0.94398680\n",
      "Iteration 9, loss = 0.94365397\n",
      "Iteration 10, loss = 0.90351833\n",
      "Iteration 11, loss = 0.86349416\n",
      "Iteration 10, loss = 0.90918423\n",
      "Iteration 10, loss = 0.90897680\n",
      "Iteration 10, loss = 0.90394256\n",
      "Iteration 12, loss = 0.81964027\n",
      "Iteration 11, loss = 0.86957428\n",
      "Iteration 11, loss = 0.86383296\n",
      "Iteration 11, loss = 0.86963488\n",
      "Iteration 13, loss = 0.77647240\n",
      "Iteration 14, loss = 0.73710468\n",
      "Iteration 12, loss = 0.82589514\n",
      "Iteration 12, loss = 0.81997544\n",
      "Iteration 12, loss = 0.82635954\n",
      "Iteration 13, loss = 0.77719895\n",
      "Iteration 13, loss = 0.78248863\n",
      "Iteration 14, loss = 0.74304582\n",
      "Iteration 15, loss = 0.69698524\n",
      "Iteration 13, loss = 0.78292738\n",
      "Iteration 14, loss = 0.73787028\n",
      "Iteration 15, loss = 0.70261714\n",
      "Iteration 14, loss = 0.74378274\n",
      "Iteration 16, loss = 0.66208152\n",
      "Iteration 15, loss = 0.69774332\n",
      "Iteration 16, loss = 0.65697180\n",
      "Iteration 16, loss = 0.65775031\n",
      "Iteration 15, loss = 0.70415456\n",
      "Iteration 17, loss = 0.61874098\n",
      "Iteration 17, loss = 0.62312027\n",
      "Iteration 18, loss = 0.58352575\n",
      "Iteration 16, loss = 0.66436206\n",
      "Iteration 19, loss = 0.55198134\n",
      "Iteration 18, loss = 0.58699481\n",
      "Iteration 17, loss = 0.62610544\n",
      "Iteration 20, loss = 0.52404394\n",
      "Iteration 21, loss = 0.49922234\n",
      "Iteration 19, loss = 0.55432974\n",
      "Iteration 18, loss = 0.59057179\n",
      "Iteration 17, loss = 0.61960460\n",
      "Iteration 20, loss = 0.52521393\n",
      "Iteration 22, loss = 0.47703490\n",
      "Iteration 19, loss = 0.55851117\n",
      "Iteration 18, loss = 0.58448673\n",
      "Iteration 21, loss = 0.49924018\n",
      "Iteration 20, loss = 0.52996153\n",
      "Iteration 22, loss = 0.47598413\n",
      "Iteration 21, loss = 0.50452965\n",
      "Iteration 19, loss = 0.55296703\n",
      "Iteration 23, loss = 0.45508703\n",
      "Iteration 20, loss = 0.52496468\n",
      "Iteration 23, loss = 0.45705205\n",
      "Iteration 24, loss = 0.43626499\n",
      "Iteration 21, loss = 0.50006240\n",
      "Iteration 22, loss = 0.48173613\n",
      "Iteration 25, loss = 0.41927411\n",
      "Iteration 26, loss = 0.40378006\n",
      "Iteration 23, loss = 0.46127367\n",
      "Iteration 22, loss = 0.47784870\n",
      "Iteration 23, loss = 0.45794681\n",
      "Iteration 24, loss = 0.44294131\n",
      "Iteration 24, loss = 0.43906644\n",
      "Iteration 24, loss = 0.44012414\n",
      "Iteration 25, loss = 0.42283502\n",
      "Iteration 25, loss = 0.42650299\n",
      "Iteration 25, loss = 0.42412980\n",
      "Iteration 26, loss = 0.40798767\n",
      "Iteration 27, loss = 0.38943376\n",
      "Iteration 27, loss = 0.39417677\n",
      "Iteration 26, loss = 0.40959577\n",
      "Iteration 26, loss = 0.41153055\n",
      "Iteration 27, loss = 0.39611444\n",
      "Iteration 27, loss = 0.39762875\n",
      "Iteration 28, loss = 0.38112378\n",
      "Iteration 28, loss = 0.38335100\n",
      "Iteration 28, loss = 0.37591802\n",
      "Iteration 29, loss = 0.36862738\n",
      "Iteration 29, loss = 0.36298147\n",
      "Iteration 29, loss = 0.37116750\n",
      "Iteration 28, loss = 0.38450290\n",
      "Iteration 29, loss = 0.37196771\n",
      "Iteration 30, loss = 0.35051794\n",
      "Iteration 30, loss = 0.35664597\n",
      "Iteration 30, loss = 0.35938562\n",
      "Iteration 31, loss = 0.34511113\n",
      "Iteration 31, loss = 0.33837678\n",
      "Iteration 31, loss = 0.34800428\n",
      "Iteration 30, loss = 0.35982663\n",
      "Iteration 32, loss = 0.32664586\n",
      "Iteration 32, loss = 0.33390250\n",
      "Iteration 32, loss = 0.33691459\n",
      "Iteration 31, loss = 0.34809084\n",
      "Iteration 33, loss = 0.31523259\n",
      "Iteration 33, loss = 0.32303850\n",
      "Iteration 34, loss = 0.30412708\n",
      "Iteration 32, loss = 0.33667889\n",
      "Iteration 33, loss = 0.32605828\n",
      "Iteration 33, loss = 0.32559088\n",
      "Iteration 34, loss = 0.31246040\n",
      "Iteration 35, loss = 0.29339409\n",
      "Iteration 34, loss = 0.31549152\n",
      "Iteration 34, loss = 0.31485520\n",
      "Iteration 35, loss = 0.30523414\n",
      "Iteration 35, loss = 0.30217645\n",
      "Iteration 36, loss = 0.28307540\n",
      "Iteration 35, loss = 0.30444716\n",
      "Iteration 36, loss = 0.29214644\n",
      "Iteration 36, loss = 0.29428924\n",
      "Iteration 37, loss = 0.27314254\n",
      "Iteration 36, loss = 0.29528517\n",
      "Iteration 37, loss = 0.28239169\n",
      "Iteration 38, loss = 0.26354107\n",
      "Iteration 38, loss = 0.27293947\n",
      "Iteration 37, loss = 0.28437457\n",
      "Iteration 37, loss = 0.28563947\n",
      "Iteration 39, loss = 0.25434140\n",
      "Iteration 38, loss = 0.27477351\n",
      "Iteration 39, loss = 0.26379638\n",
      "Iteration 38, loss = 0.27630310\n",
      "Iteration 40, loss = 0.24557817\n",
      "Iteration 39, loss = 0.26551274\n",
      "Iteration 40, loss = 0.25496135\n",
      "Iteration 40, loss = 0.25658987\n",
      "Iteration 41, loss = 0.23731733\n",
      "Iteration 41, loss = 0.24645300\n",
      "Iteration 39, loss = 0.26729114\n",
      "Iteration 42, loss = 0.22962566\n",
      "Iteration 43, loss = 0.22242365\n",
      "Iteration 41, loss = 0.24800505\n",
      "Iteration 42, loss = 0.23825927\n",
      "Iteration 44, loss = 0.21560386\n",
      "Iteration 42, loss = 0.23981623\n",
      "Iteration 45, loss = 0.20921299\n",
      "Iteration 43, loss = 0.23038318\n",
      "Iteration 40, loss = 0.25861043\n",
      "Iteration 44, loss = 0.22289631\n",
      "Iteration 43, loss = 0.23190085\n",
      "Iteration 44, loss = 0.22434684\n",
      "Iteration 41, loss = 0.25023969\n",
      "Iteration 46, loss = 0.20325571\n",
      "Iteration 45, loss = 0.21592096\n",
      "Iteration 47, loss = 0.19769961\n",
      "Iteration 42, loss = 0.24230680\n",
      "Iteration 46, loss = 0.20942916\n",
      "Iteration 43, loss = 0.23493398\n",
      "Iteration 48, loss = 0.19250652\n",
      "Iteration 45, loss = 0.21730614\n",
      "Iteration 44, loss = 0.22807300\n",
      "Iteration 49, loss = 0.18764840\n",
      "Iteration 47, loss = 0.20331122\n",
      "Iteration 46, loss = 0.21075336\n",
      "Iteration 45, loss = 0.22160117\n",
      "Iteration 46, loss = 0.21546258\n",
      "Iteration 50, loss = 0.18306683\n",
      "Iteration 48, loss = 0.19750934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 0.20969509\n",
      "Iteration 49, loss = 0.19206628\n",
      "Iteration 47, loss = 0.20456164\n",
      "Iteration 48, loss = 0.20425430\n",
      "Iteration 50, loss = 0.18695820\n",
      "Iteration 49, loss = 0.19912391\n",
      "Iteration 48, loss = 0.19870232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12359203\n",
      "Iteration 50, loss = 0.19429029\n",
      "Iteration 49, loss = 0.19321325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.10547603\n",
      "Iteration 3, loss = 1.08520683\n",
      "Iteration 1, loss = 1.12155332\n",
      "Iteration 1, loss = 1.12297939\n",
      "Iteration 4, loss = 1.06507520\n",
      "Iteration 2, loss = 1.10319245\n",
      "Iteration 50, loss = 0.18806515\n",
      "Iteration 5, loss = 1.04534234\n",
      "Iteration 2, loss = 1.10463957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.08265373\n",
      "Iteration 6, loss = 1.02391436\n",
      "Iteration 3, loss = 1.08387791\n",
      "Iteration 9, loss = 0.94291364\n",
      "Iteration 7, loss = 1.00010891\n",
      "Iteration 4, loss = 1.06339086\n",
      "Iteration 5, loss = 1.04322742\n",
      "Iteration 8, loss = 0.97338921\n",
      "Iteration 6, loss = 1.02133471\n",
      "Iteration 1, loss = 1.12169637\n",
      "Iteration 2, loss = 1.10367643\n",
      "Iteration 10, loss = 0.90790360\n",
      "Iteration 7, loss = 0.99712922\n",
      "Iteration 8, loss = 0.97004273\n",
      "Iteration 3, loss = 1.08327093\n",
      "Iteration 4, loss = 1.06240810\n",
      "Iteration 11, loss = 0.86818148\n",
      "Iteration 9, loss = 0.93908460\n",
      "Iteration 12, loss = 0.82436982\n",
      "Iteration 5, loss = 1.04243206\n",
      "Iteration 4, loss = 1.06299021\n",
      "Iteration 13, loss = 0.78038994\n",
      "Iteration 6, loss = 1.02072121\n",
      "Iteration 10, loss = 0.90349662\n",
      "Iteration 7, loss = 0.99681458\n",
      "Iteration 14, loss = 0.74104064\n",
      "Iteration 5, loss = 1.04292396\n",
      "Iteration 8, loss = 0.96990563\n",
      "Iteration 15, loss = 0.70077320\n",
      "Iteration 11, loss = 0.86314300\n",
      "Iteration 9, loss = 0.93919238\n",
      "Iteration 6, loss = 1.02106935\n",
      "Iteration 16, loss = 0.66036051\n",
      "Iteration 12, loss = 0.81905126\n",
      "Iteration 10, loss = 0.90393441\n",
      "Iteration 17, loss = 0.62154557\n",
      "Iteration 11, loss = 0.86400907\n",
      "Iteration 7, loss = 0.99701522\n",
      "Iteration 18, loss = 0.58563791\n",
      "Iteration 12, loss = 0.82031007\n",
      "Iteration 13, loss = 0.77577536\n",
      "Iteration 13, loss = 0.77744713\n",
      "Iteration 14, loss = 0.73847681\n",
      "Iteration 14, loss = 0.73598468\n",
      "Iteration 8, loss = 0.96992150\n",
      "Iteration 19, loss = 0.55327501\n",
      "Iteration 9, loss = 0.93899659\n",
      "Iteration 15, loss = 0.69841897\n",
      "Iteration 15, loss = 0.69517228\n",
      "Iteration 10, loss = 0.90349177\n",
      "Iteration 16, loss = 0.65445244\n",
      "Iteration 20, loss = 0.52447624\n",
      "Iteration 16, loss = 0.65839172\n",
      "Iteration 11, loss = 0.86328055\n",
      "Iteration 17, loss = 0.61551190\n",
      "Iteration 12, loss = 0.81934881\n",
      "Iteration 17, loss = 0.62007001\n",
      "Iteration 21, loss = 0.49878956\n",
      "Iteration 13, loss = 0.77663159\n",
      "Iteration 18, loss = 0.58472007\n",
      "Iteration 18, loss = 0.57964325\n",
      "Iteration 22, loss = 0.47575375\n",
      "Iteration 14, loss = 0.73719170\n",
      "Iteration 19, loss = 0.55298408\n",
      "Iteration 23, loss = 0.45498928\n",
      "Iteration 19, loss = 0.54734426\n",
      "Iteration 15, loss = 0.69686816\n",
      "Iteration 20, loss = 0.51860056\n",
      "Iteration 24, loss = 0.43627176\n",
      "Iteration 20, loss = 0.52477394\n",
      "Iteration 16, loss = 0.65665090\n",
      "Iteration 21, loss = 0.49970749\n",
      "Iteration 17, loss = 0.61824847\n",
      "Iteration 25, loss = 0.41935853\n",
      "Iteration 22, loss = 0.47726251\n",
      "Iteration 18, loss = 0.58291011\n",
      "Iteration 21, loss = 0.49295428\n",
      "Iteration 23, loss = 0.45713003\n",
      "Iteration 26, loss = 0.40388341\n",
      "Iteration 20, loss = 0.52297246\n",
      "Iteration 19, loss = 0.55116112\n",
      "Iteration 22, loss = 0.46997655\n",
      "Iteration 24, loss = 0.43910096\n",
      "Iteration 23, loss = 0.44927648\n",
      "Iteration 27, loss = 0.38948991\n",
      "Iteration 21, loss = 0.49787212\n",
      "Iteration 24, loss = 0.43061812\n",
      "Iteration 25, loss = 0.42291472\n",
      "Iteration 22, loss = 0.47539439\n",
      "Iteration 25, loss = 0.41374922\n",
      "Iteration 28, loss = 0.37588283\n",
      "Iteration 23, loss = 0.45520287\n",
      "Iteration 26, loss = 0.39830080\n",
      "Iteration 26, loss = 0.40819394\n",
      "Iteration 29, loss = 0.36293877\n",
      "Iteration 27, loss = 0.38392752\n",
      "Iteration 27, loss = 0.39456592\n",
      "Iteration 30, loss = 0.35058490\n",
      "Iteration 24, loss = 0.43706792\n",
      "Iteration 28, loss = 0.38171731\n",
      "Iteration 25, loss = 0.42074657\n",
      "Iteration 28, loss = 0.37033258\n",
      "Iteration 31, loss = 0.33866724\n",
      "Iteration 29, loss = 0.36939368\n",
      "Iteration 26, loss = 0.40587333\n",
      "Iteration 29, loss = 0.35730726\n",
      "Iteration 32, loss = 0.32717251\n",
      "Iteration 30, loss = 0.35747910\n",
      "Iteration 27, loss = 0.39204153\n",
      "Iteration 30, loss = 0.34471982\n",
      "Iteration 31, loss = 0.34583972\n",
      "Iteration 33, loss = 0.31603566\n",
      "Iteration 32, loss = 0.33448847\n",
      "Iteration 31, loss = 0.33247891\n",
      "Iteration 28, loss = 0.37894126\n",
      "Iteration 33, loss = 0.32341784\n",
      "Iteration 32, loss = 0.32064078\n",
      "Iteration 34, loss = 0.30525397\n",
      "Iteration 29, loss = 0.36637390\n",
      "Iteration 34, loss = 0.31258866\n",
      "Iteration 33, loss = 0.30908917\n",
      "Iteration 30, loss = 0.35421415\n",
      "Iteration 35, loss = 0.29476171\n",
      "Iteration 35, loss = 0.30202968\n",
      "Iteration 34, loss = 0.29784606\n",
      "Iteration 31, loss = 0.34253644\n",
      "Iteration 36, loss = 0.28458305\n",
      "Iteration 35, loss = 0.28692361\n",
      "Iteration 32, loss = 0.33114127\n",
      "Iteration 36, loss = 0.27630526\n",
      "Iteration 37, loss = 0.27472916\n",
      "Iteration 36, loss = 0.29173942\n",
      "Iteration 37, loss = 0.26599917\n",
      "Iteration 33, loss = 0.32004674\n",
      "Iteration 38, loss = 0.26526441\n",
      "Iteration 34, loss = 0.30926977\n",
      "Iteration 39, loss = 0.25616369\n",
      "Iteration 38, loss = 0.25610094\n",
      "Iteration 37, loss = 0.28171506\n",
      "Iteration 35, loss = 0.29881686\n",
      "Iteration 39, loss = 0.24657003\n",
      "Iteration 38, loss = 0.27199524\n",
      "Iteration 36, loss = 0.28863716\n",
      "Iteration 40, loss = 0.24742888\n",
      "Iteration 39, loss = 0.26257590\n",
      "Iteration 40, loss = 0.23750305\n",
      "Iteration 37, loss = 0.27875404\n",
      "Iteration 40, loss = 0.25347506\n",
      "Iteration 41, loss = 0.22907031\n",
      "Iteration 38, loss = 0.26913760\n",
      "Iteration 41, loss = 0.24468606\n",
      "Iteration 41, loss = 0.23900556\n",
      "Iteration 39, loss = 0.25981269\n",
      "Iteration 42, loss = 0.23616209\n",
      "Iteration 42, loss = 0.23089409\n",
      "Iteration 42, loss = 0.22116494\n",
      "Iteration 40, loss = 0.25078553\n",
      "Iteration 43, loss = 0.22800965\n",
      "Iteration 43, loss = 0.22324198\n",
      "Iteration 41, loss = 0.24215928\n",
      "Iteration 44, loss = 0.21618223\n",
      "Iteration 43, loss = 0.21365109\n",
      "Iteration 44, loss = 0.22030794\n",
      "Iteration 42, loss = 0.23412556\n",
      "Iteration 44, loss = 0.20660463\n",
      "Iteration 45, loss = 0.20955462\n",
      "Iteration 43, loss = 0.22661319\n",
      "Iteration 45, loss = 0.21309975\n",
      "Iteration 44, loss = 0.21949652\n",
      "Iteration 46, loss = 0.20329273\n",
      "Iteration 46, loss = 0.20633883\n",
      "Iteration 45, loss = 0.20000572\n",
      "Iteration 47, loss = 0.19988314\n",
      "Iteration 45, loss = 0.21281443\n",
      "Iteration 48, loss = 0.19379546\n",
      "Iteration 46, loss = 0.19384155\n",
      "Iteration 47, loss = 0.19741025\n",
      "Iteration 46, loss = 0.20654681\n",
      "Iteration 49, loss = 0.18806747\n",
      "Iteration 47, loss = 0.18805809\n",
      "Iteration 48, loss = 0.19189646\n",
      "Iteration 48, loss = 0.18263743\n",
      "Iteration 47, loss = 0.20067714\n",
      "Iteration 49, loss = 0.17751531\n",
      "Iteration 49, loss = 0.18672790\n",
      "Iteration 50, loss = 0.17272301\n",
      "Iteration 48, loss = 0.19515573\n",
      "Iteration 50, loss = 0.18266534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n",
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.18185811\n",
      "Iteration 1, loss = 1.12296168\n",
      "Iteration 4, loss = 1.05936786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 0.18996647\n",
      "Iteration 1, loss = 1.17034185\n",
      "Iteration 50, loss = 0.18504525\n",
      "Iteration 2, loss = 1.15697159\n",
      "Iteration 2, loss = 1.10489581\n",
      "Iteration 3, loss = 1.14015443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 1.12191133\n",
      "Iteration 1, loss = 1.17044684\n",
      "Iteration 3, loss = 1.08451156\n",
      "Iteration 1, loss = 1.24342598\n",
      "Iteration 5, loss = 1.10363937\n",
      "Iteration 2, loss = 1.15705592\n",
      "Iteration 2, loss = 1.16138892\n",
      "Iteration 6, loss = 1.08571040\n",
      "Iteration 3, loss = 1.14021956\n",
      "Iteration 3, loss = 1.09672949\n",
      "Iteration 4, loss = 1.06428622\n",
      "Iteration 7, loss = 1.06892760\n",
      "Iteration 5, loss = 1.04437619\n",
      "Iteration 4, loss = 1.12196578\n",
      "Iteration 6, loss = 1.02303281\n",
      "Iteration 5, loss = 1.03862548\n",
      "Iteration 8, loss = 1.05903235\n",
      "Iteration 9, loss = 1.05505613\n",
      "Iteration 5, loss = 1.10369770\n",
      "Iteration 6, loss = 1.02091155\n",
      "Iteration 10, loss = 1.05055627\n",
      "Iteration 7, loss = 0.99309769\n",
      "Iteration 6, loss = 1.08579092\n",
      "Iteration 11, loss = 1.04395091\n",
      "Iteration 8, loss = 0.94987965\n",
      "Iteration 12, loss = 1.03388123\n",
      "Iteration 7, loss = 1.06871762\n",
      "Iteration 9, loss = 0.89979980\n",
      "Iteration 13, loss = 1.01963558\n",
      "Iteration 7, loss = 0.99920381\n",
      "Iteration 10, loss = 0.84413485\n",
      "Iteration 14, loss = 1.00257037\n",
      "Iteration 8, loss = 1.05882414\n",
      "Iteration 11, loss = 0.78028364\n",
      "Iteration 8, loss = 0.97247488\n",
      "Iteration 15, loss = 0.98392746\n",
      "Iteration 12, loss = 0.70890293\n",
      "Iteration 9, loss = 0.94194496\n",
      "Iteration 9, loss = 1.05484799\n",
      "Iteration 13, loss = 0.63363784\n",
      "Iteration 16, loss = 0.96205890\n",
      "Iteration 10, loss = 0.90686780\n",
      "Iteration 10, loss = 1.05042560\n",
      "Iteration 14, loss = 0.55998401\n",
      "Iteration 11, loss = 0.86707303\n",
      "Iteration 17, loss = 0.93651730\n",
      "Iteration 11, loss = 1.04377333\n",
      "Iteration 12, loss = 0.82327605\n",
      "Iteration 15, loss = 0.49439842\n",
      "Iteration 18, loss = 0.90810504\n",
      "Iteration 13, loss = 0.77898319\n",
      "Iteration 12, loss = 1.03357931\n",
      "Iteration 16, loss = 0.43847420\n",
      "Iteration 19, loss = 0.87826606\n",
      "Iteration 14, loss = 0.73967124\n",
      "Iteration 15, loss = 0.69980851\n",
      "Iteration 20, loss = 0.84589517\n",
      "Iteration 13, loss = 1.01926598\n",
      "Iteration 17, loss = 0.39231021\n",
      "Iteration 18, loss = 0.35451170\n",
      "Iteration 14, loss = 1.00172733\n",
      "Iteration 21, loss = 0.81100905\n",
      "Iteration 16, loss = 0.66006784\n",
      "Iteration 17, loss = 0.62210623\n",
      "Iteration 22, loss = 0.77503271\n",
      "Iteration 19, loss = 0.32251145\n",
      "Iteration 15, loss = 0.98231069\n",
      "Iteration 23, loss = 0.73918641\n",
      "Iteration 18, loss = 0.58707418\n",
      "Iteration 16, loss = 0.96010431\n",
      "Iteration 20, loss = 0.29431053\n",
      "Iteration 19, loss = 0.55573157\n",
      "Iteration 24, loss = 0.70442719\n",
      "Iteration 20, loss = 0.52802737\n",
      "Iteration 25, loss = 0.67171330\n",
      "Iteration 21, loss = 0.26784254\n",
      "Iteration 17, loss = 0.93391607\n",
      "Iteration 22, loss = 0.24253641\n",
      "Iteration 21, loss = 0.50354950\n",
      "Iteration 18, loss = 0.90591086\n",
      "Iteration 26, loss = 0.64133106\n",
      "Iteration 23, loss = 0.21870172\n",
      "Iteration 22, loss = 0.48184421\n",
      "Iteration 24, loss = 0.19652015\n",
      "Iteration 27, loss = 0.61342198\n",
      "Iteration 19, loss = 0.87578604\n",
      "Iteration 23, loss = 0.46253426\n",
      "Iteration 25, loss = 0.17704677\n",
      "Iteration 26, loss = 0.16046056\n",
      "Iteration 20, loss = 0.84265293\n",
      "Iteration 28, loss = 0.58773140\n",
      "Iteration 24, loss = 0.44536585\n",
      "Iteration 27, loss = 0.14637882\n",
      "Iteration 29, loss = 0.56398737\n",
      "Iteration 28, loss = 0.13432628\n",
      "Iteration 21, loss = 0.80728085\n",
      "Iteration 25, loss = 0.43003280\n",
      "Iteration 30, loss = 0.54197186\n",
      "Iteration 29, loss = 0.12399090\n",
      "Iteration 31, loss = 0.52162656\n",
      "Iteration 26, loss = 0.41613813\n",
      "Iteration 22, loss = 0.77118125\n",
      "Iteration 30, loss = 0.11518560\n",
      "Iteration 23, loss = 0.73551722\n",
      "Iteration 32, loss = 0.50270658\n",
      "Iteration 27, loss = 0.40328506\n",
      "Iteration 24, loss = 0.70108514\n",
      "Iteration 33, loss = 0.48483308\n",
      "Iteration 28, loss = 0.39113956\n",
      "Iteration 31, loss = 0.10766231\n",
      "Iteration 25, loss = 0.66862642\n",
      "Iteration 29, loss = 0.37948032\n",
      "Iteration 26, loss = 0.63842298\n",
      "Iteration 32, loss = 0.10124247\n",
      "Iteration 34, loss = 0.46782534\n",
      "Iteration 30, loss = 0.36816277\n",
      "Iteration 35, loss = 0.45152573\n",
      "Iteration 31, loss = 0.35709442\n",
      "Iteration 36, loss = 0.43589520\n",
      "Iteration 27, loss = 0.61036269\n",
      "Iteration 33, loss = 0.09574423\n",
      "Iteration 32, loss = 0.34629843\n",
      "Iteration 28, loss = 0.58451344\n",
      "Iteration 34, loss = 0.09101769\n",
      "Iteration 33, loss = 0.33568433\n",
      "Iteration 29, loss = 0.56056964\n",
      "Iteration 34, loss = 0.32523014\n",
      "Iteration 35, loss = 0.08693441\n",
      "Iteration 35, loss = 0.31499816\n",
      "Iteration 30, loss = 0.53851102\n",
      "Iteration 36, loss = 0.08338301\n",
      "Iteration 36, loss = 0.30502118\n",
      "Iteration 31, loss = 0.51820500\n",
      "Iteration 37, loss = 0.08027356\n",
      "Iteration 37, loss = 0.29530279\n",
      "Iteration 32, loss = 0.49907951\n",
      "Iteration 38, loss = 0.28582758\n",
      "Iteration 38, loss = 0.07753383\n",
      "Iteration 39, loss = 0.27662709\n",
      "Iteration 33, loss = 0.48096345\n",
      "Iteration 39, loss = 0.07509974\n",
      "Iteration 40, loss = 0.26771607\n",
      "Iteration 34, loss = 0.46369933\n",
      "Iteration 41, loss = 0.25906364\n",
      "Iteration 40, loss = 0.07291792\n",
      "Iteration 35, loss = 0.44718872\n",
      "Iteration 42, loss = 0.25079502\n",
      "Iteration 41, loss = 0.07095490\n",
      "Iteration 36, loss = 0.43126178\n",
      "Iteration 37, loss = 0.42076029\n",
      "Iteration 43, loss = 0.24302893\n",
      "Iteration 42, loss = 0.06918270\n",
      "Iteration 38, loss = 0.40603174\n",
      "Iteration 43, loss = 0.06758486\n",
      "Iteration 44, loss = 0.23574711\n",
      "Iteration 37, loss = 0.41581564\n",
      "Iteration 45, loss = 0.22880494\n",
      "Iteration 44, loss = 0.06613074\n",
      "Iteration 38, loss = 0.40075934\n",
      "Iteration 46, loss = 0.22218056\n",
      "Iteration 39, loss = 0.39163764\n",
      "Iteration 39, loss = 0.38600073\n",
      "Iteration 45, loss = 0.06480332\n",
      "Iteration 40, loss = 0.37750668\n",
      "Iteration 47, loss = 0.21595831\n",
      "Iteration 46, loss = 0.06359100\n",
      "Iteration 40, loss = 0.37151789\n",
      "Iteration 48, loss = 0.21010382\n",
      "Iteration 41, loss = 0.36361377\n",
      "Iteration 41, loss = 0.35726695\n",
      "Iteration 42, loss = 0.34324598\n",
      "Iteration 47, loss = 0.06248063\n",
      "Iteration 42, loss = 0.35001311\n",
      "Iteration 49, loss = 0.20459859\n",
      "Iteration 43, loss = 0.33666053\n",
      "Iteration 48, loss = 0.06146010\n",
      "Iteration 43, loss = 0.32945371\n",
      "Iteration 44, loss = 0.32356266\n",
      "Iteration 50, loss = 0.19940156\n",
      "Iteration 45, loss = 0.31083261\n",
      "Iteration 49, loss = 0.06052176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44, loss = 0.31588589\n",
      "Iteration 50, loss = 0.05965777\n",
      "Iteration 46, loss = 0.29842910\n",
      "Iteration 45, loss = 0.30260710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.28961512\n",
      "Iteration 47, loss = 0.28640043\n",
      "Iteration 47, loss = 0.27692673\n",
      "Iteration 48, loss = 0.27475837\n",
      "Iteration 48, loss = 0.26455743\n",
      "Iteration 49, loss = 0.26357853\n",
      "Iteration 1, loss = 1.24330395\n",
      "Iteration 49, loss = 0.25261358\n",
      "Iteration 50, loss = 0.25288392\n",
      "Iteration 50, loss = 0.24109770\n",
      "Iteration 2, loss = 1.16304381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n",
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.09908913\n",
      "Iteration 4, loss = 1.06123291\n",
      "Iteration 1, loss = 1.12234407\n",
      "Iteration 1, loss = 1.17009927\n",
      "Iteration 1, loss = 1.17038216\n",
      "Iteration 5, loss = 1.03997471\n",
      "Iteration 2, loss = 1.15675957\n",
      "Iteration 2, loss = 1.10378055\n",
      "Iteration 2, loss = 1.15702750\n",
      "Iteration 6, loss = 1.02170612\n",
      "Iteration 3, loss = 1.08283460\n",
      "Iteration 3, loss = 1.13997367\n",
      "Iteration 4, loss = 1.12177846\n",
      "Iteration 7, loss = 0.99319124\n",
      "Iteration 3, loss = 1.14021794\n",
      "Iteration 4, loss = 1.06200969\n",
      "Iteration 5, loss = 1.10361308\n",
      "Iteration 4, loss = 1.12196294\n",
      "Iteration 8, loss = 0.95004845\n",
      "Iteration 5, loss = 1.04134472\n",
      "Iteration 6, loss = 1.08565041\n",
      "Iteration 5, loss = 1.10369627\n",
      "Iteration 7, loss = 1.06853878\n",
      "Iteration 9, loss = 0.90115639\n",
      "Iteration 6, loss = 1.01890274\n",
      "Iteration 6, loss = 1.08574790\n",
      "Iteration 8, loss = 1.05902946\n",
      "Iteration 7, loss = 1.06868839\n",
      "Iteration 10, loss = 0.84523274\n",
      "Iteration 7, loss = 0.99432940\n",
      "Iteration 8, loss = 1.05874557\n",
      "Iteration 8, loss = 0.96684534\n",
      "Iteration 9, loss = 1.05508535\n",
      "Iteration 11, loss = 0.78124956\n",
      "Iteration 9, loss = 1.05467792\n",
      "Iteration 10, loss = 1.05053921\n",
      "Iteration 9, loss = 0.93549177\n",
      "Iteration 12, loss = 0.70978366\n",
      "Iteration 11, loss = 1.04383885\n",
      "Iteration 10, loss = 0.89955059\n",
      "Iteration 10, loss = 1.05011219\n",
      "Iteration 13, loss = 0.63447124\n",
      "Iteration 11, loss = 0.85893342\n",
      "Iteration 12, loss = 1.03374254\n",
      "Iteration 11, loss = 1.04337991\n",
      "Iteration 12, loss = 0.81475610\n",
      "Iteration 14, loss = 0.56128294\n",
      "Iteration 13, loss = 1.01956565\n",
      "Iteration 12, loss = 1.03321798\n",
      "Iteration 13, loss = 0.77205603\n",
      "Iteration 13, loss = 1.01891204\n",
      "Iteration 15, loss = 0.49603950\n",
      "Iteration 16, loss = 0.44043909\n",
      "Iteration 14, loss = 1.00111552\n",
      "Iteration 17, loss = 0.39477091\n",
      "Iteration 15, loss = 0.98165764\n",
      "Iteration 18, loss = 0.35790955\n",
      "Iteration 19, loss = 0.32692925\n",
      "Iteration 16, loss = 0.95935963\n",
      "Iteration 20, loss = 0.29920093\n",
      "Iteration 17, loss = 0.93351230\n",
      "Iteration 14, loss = 0.73243446\n",
      "Iteration 14, loss = 1.00213403\n",
      "Iteration 21, loss = 0.27284612\n",
      "Iteration 15, loss = 0.98282365\n",
      "Iteration 15, loss = 0.69212771\n",
      "Iteration 18, loss = 0.90598434\n",
      "Iteration 19, loss = 0.87570911\n",
      "Iteration 16, loss = 0.65215367\n",
      "Iteration 22, loss = 0.24744602\n",
      "Iteration 16, loss = 0.96069868\n",
      "Iteration 17, loss = 0.61422612\n",
      "Iteration 20, loss = 0.84226333\n",
      "Iteration 23, loss = 0.22338110\n",
      "Iteration 18, loss = 0.57946073\n",
      "Iteration 17, loss = 0.93505635\n",
      "Iteration 21, loss = 0.80675368\n",
      "Iteration 19, loss = 0.54839266\n",
      "Iteration 18, loss = 0.90742980\n",
      "Iteration 24, loss = 0.20068944\n",
      "Iteration 22, loss = 0.77042937\n",
      "Iteration 19, loss = 0.87758521\n",
      "Iteration 20, loss = 0.52089609\n",
      "Iteration 23, loss = 0.73455484\n",
      "Iteration 20, loss = 0.84440327\n",
      "Iteration 21, loss = 0.49650061\n",
      "Iteration 25, loss = 0.18065518\n",
      "Iteration 24, loss = 0.69981341\n",
      "Iteration 22, loss = 0.47475992\n",
      "Iteration 21, loss = 0.80899114\n",
      "Iteration 25, loss = 0.66687497\n",
      "Iteration 23, loss = 0.45529498\n",
      "Iteration 22, loss = 0.77254795\n",
      "Iteration 26, loss = 0.16336221\n",
      "Iteration 26, loss = 0.63614174\n",
      "Iteration 24, loss = 0.43789231\n",
      "Iteration 27, loss = 0.60777940\n",
      "Iteration 27, loss = 0.14839221\n",
      "Iteration 23, loss = 0.73649013\n",
      "Iteration 25, loss = 0.42229192\n",
      "Iteration 28, loss = 0.13537838\n",
      "Iteration 28, loss = 0.58157675\n",
      "Iteration 26, loss = 0.40811470\n",
      "Iteration 29, loss = 0.55731614\n",
      "Iteration 24, loss = 0.70170232\n",
      "Iteration 29, loss = 0.12405902\n",
      "Iteration 27, loss = 0.39499160\n",
      "Iteration 30, loss = 0.53476407\n",
      "Iteration 25, loss = 0.66887124\n",
      "Iteration 28, loss = 0.38257609\n",
      "Iteration 31, loss = 0.51394691\n",
      "Iteration 26, loss = 0.63823827\n",
      "Iteration 32, loss = 0.49441621\n",
      "Iteration 30, loss = 0.11431180\n",
      "Iteration 29, loss = 0.37070536\n",
      "Iteration 27, loss = 0.61001761\n",
      "Iteration 31, loss = 0.10594267\n",
      "Iteration 33, loss = 0.47589764\n",
      "Iteration 30, loss = 0.35919064\n",
      "Iteration 28, loss = 0.58392469\n",
      "Iteration 31, loss = 0.34797383\n",
      "Iteration 29, loss = 0.55975254\n",
      "Iteration 32, loss = 0.09877401\n",
      "Iteration 33, loss = 0.09264232\n",
      "Iteration 34, loss = 0.45820240\n",
      "Iteration 32, loss = 0.33703022\n",
      "Iteration 30, loss = 0.53746645\n",
      "Iteration 34, loss = 0.08739111\n",
      "Iteration 35, loss = 0.44120256\n",
      "Iteration 31, loss = 0.51690039\n",
      "Iteration 36, loss = 0.42484662\n",
      "Iteration 35, loss = 0.08287969\n",
      "Iteration 33, loss = 0.32628949\n",
      "Iteration 37, loss = 0.40898536\n",
      "Iteration 32, loss = 0.49753111\n",
      "Iteration 38, loss = 0.39354748\n",
      "Iteration 36, loss = 0.07899306\n",
      "Iteration 33, loss = 0.47918233\n",
      "Iteration 34, loss = 0.31586408\n",
      "Iteration 39, loss = 0.37848143\n",
      "Iteration 34, loss = 0.46168856\n",
      "Iteration 37, loss = 0.07563277\n",
      "Iteration 35, loss = 0.30572124\n",
      "Iteration 40, loss = 0.36369867\n",
      "Iteration 35, loss = 0.44493592\n",
      "Iteration 38, loss = 0.07271836\n",
      "Iteration 36, loss = 0.29584763\n",
      "Iteration 41, loss = 0.34922877\n",
      "Iteration 36, loss = 0.42880976\n",
      "Iteration 39, loss = 0.07018037\n",
      "Iteration 42, loss = 0.33506215\n",
      "Iteration 37, loss = 0.41317066\n",
      "Iteration 37, loss = 0.28622123\n",
      "Iteration 40, loss = 0.06796294\n",
      "Iteration 38, loss = 0.39793185\n",
      "Iteration 43, loss = 0.32118479\n",
      "Iteration 41, loss = 0.06601915\n",
      "Iteration 38, loss = 0.27689672\n",
      "Iteration 39, loss = 0.38302174\n",
      "Iteration 39, loss = 0.26788626\n",
      "Iteration 42, loss = 0.06430919\n",
      "Iteration 44, loss = 0.30761373\n",
      "Iteration 40, loss = 0.36843013\n",
      "Iteration 40, loss = 0.25915489\n",
      "Iteration 43, loss = 0.06279687\n",
      "Iteration 45, loss = 0.29450068\n",
      "Iteration 41, loss = 0.35407197\n",
      "Iteration 41, loss = 0.25071865\n",
      "Iteration 44, loss = 0.06145560\n",
      "Iteration 42, loss = 0.33998874\n",
      "Iteration 46, loss = 0.28175922\n",
      "Iteration 43, loss = 0.32618727\n",
      "Iteration 47, loss = 0.26945662\n",
      "Iteration 45, loss = 0.06026171\n",
      "Iteration 42, loss = 0.24267142\n",
      "Iteration 44, loss = 0.31265776\n",
      "Iteration 45, loss = 0.29947232\n",
      "Iteration 48, loss = 0.25763433\n",
      "Iteration 46, loss = 0.05919572\n",
      "Iteration 46, loss = 0.28662981\n",
      "Iteration 43, loss = 0.23514609\n",
      "Iteration 47, loss = 0.05823972\n",
      "Iteration 49, loss = 0.24636291\n",
      "Iteration 44, loss = 0.22811885\n",
      "Iteration 48, loss = 0.05738020\n",
      "Iteration 47, loss = 0.27415762\n",
      "Iteration 50, loss = 0.23561394\n",
      "Iteration 48, loss = 0.26213356\n",
      "Iteration 45, loss = 0.22145581\n",
      "Iteration 49, loss = 0.05660365\n",
      "Iteration 46, loss = 0.21514877\n",
      "Iteration 50, loss = 0.05590044\n",
      "Iteration 49, loss = 0.25057807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 0.20921953\n",
      "Iteration 48, loss = 0.20365374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.23954763\n",
      "Iteration 49, loss = 0.19841685\n",
      "Iteration 1, loss = 1.24472672\n",
      "Iteration 50, loss = 0.19347780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n",
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.16270465\n",
      "Iteration 3, loss = 1.09818713\n",
      "Iteration 1, loss = 1.16987252\n",
      "Iteration 4, loss = 1.06089763\n",
      "Iteration 1, loss = 1.17090686\n",
      "Iteration 5, loss = 1.04018480\n",
      "Iteration 2, loss = 1.15649729\n",
      "Iteration 2, loss = 1.15753753\n",
      "Iteration 6, loss = 1.02240203\n",
      "Iteration 1, loss = 1.17055787\n",
      "Iteration 7, loss = 0.99399319\n",
      "Iteration 3, loss = 1.13966103\n",
      "Iteration 3, loss = 1.14073550\n",
      "Iteration 8, loss = 0.95057862\n",
      "Iteration 4, loss = 1.12253424\n",
      "Iteration 2, loss = 1.15720554\n",
      "Iteration 9, loss = 0.90194959\n",
      "Iteration 5, loss = 1.10434956\n",
      "Iteration 10, loss = 0.84624537\n",
      "Iteration 4, loss = 1.12136892\n",
      "Iteration 3, loss = 1.14040459\n",
      "Iteration 6, loss = 1.08661786\n",
      "Iteration 4, loss = 1.12217855\n",
      "Iteration 5, loss = 1.10300287\n",
      "Iteration 11, loss = 0.78233526\n",
      "Iteration 12, loss = 0.71085727\n",
      "Iteration 6, loss = 1.08498575\n",
      "Iteration 7, loss = 1.06990559\n",
      "Iteration 8, loss = 1.05926181\n",
      "Iteration 5, loss = 1.10395087\n",
      "Iteration 13, loss = 0.63533712\n",
      "Iteration 7, loss = 1.06800308\n",
      "Iteration 9, loss = 1.05532000\n",
      "Iteration 14, loss = 0.56222842\n",
      "Iteration 6, loss = 1.08605540\n",
      "Iteration 8, loss = 1.05845164\n",
      "Iteration 10, loss = 1.05098363\n",
      "Iteration 15, loss = 0.49724946\n",
      "Iteration 7, loss = 1.06904335\n",
      "Iteration 9, loss = 1.05425331\n",
      "Iteration 16, loss = 0.44160005\n",
      "Iteration 11, loss = 1.04454174\n",
      "Iteration 8, loss = 1.05896417\n",
      "Iteration 10, loss = 1.04956631\n",
      "Iteration 12, loss = 1.03466534\n",
      "Iteration 17, loss = 0.39581759\n",
      "Iteration 9, loss = 1.05505195\n",
      "Iteration 13, loss = 1.02069244\n",
      "Iteration 11, loss = 1.04268589\n",
      "Iteration 14, loss = 1.00378235\n",
      "Iteration 12, loss = 1.03236691\n",
      "Iteration 18, loss = 0.35856430\n",
      "Iteration 10, loss = 1.05059007\n",
      "Iteration 11, loss = 1.04395661\n",
      "Iteration 19, loss = 0.32729301\n",
      "Iteration 15, loss = 0.98535855\n",
      "Iteration 13, loss = 1.01785106\n",
      "Iteration 16, loss = 0.96393435\n",
      "Iteration 20, loss = 0.29966838\n",
      "Iteration 12, loss = 1.03389741\n",
      "Iteration 14, loss = 1.00038441\n",
      "Iteration 17, loss = 0.93835785\n",
      "Iteration 21, loss = 0.27393336\n",
      "Iteration 13, loss = 1.01966926\n",
      "Iteration 15, loss = 0.98110071\n",
      "Iteration 16, loss = 0.95879215\n",
      "Iteration 22, loss = 0.24918572\n",
      "Iteration 14, loss = 1.00218391\n",
      "Iteration 18, loss = 0.91013382\n",
      "Iteration 17, loss = 0.93256970\n",
      "Iteration 23, loss = 0.22547943\n",
      "Iteration 18, loss = 0.90370176\n",
      "Iteration 15, loss = 0.98308558\n",
      "Iteration 24, loss = 0.20330152\n",
      "Iteration 19, loss = 0.87338496\n",
      "Iteration 16, loss = 0.96087686\n",
      "Iteration 19, loss = 0.88074113\n",
      "Iteration 20, loss = 0.84066688\n",
      "Iteration 20, loss = 0.84818215\n",
      "Iteration 25, loss = 0.18363256\n",
      "Iteration 17, loss = 0.93510140\n",
      "Iteration 21, loss = 0.81320469\n",
      "Iteration 21, loss = 0.80564696\n",
      "Iteration 26, loss = 0.16678081\n",
      "Iteration 18, loss = 0.90729192\n",
      "Iteration 22, loss = 0.77697503\n",
      "Iteration 27, loss = 0.15231971\n",
      "Iteration 22, loss = 0.76978362\n",
      "Iteration 23, loss = 0.73440616\n",
      "Iteration 28, loss = 0.13980763\n",
      "Iteration 23, loss = 0.74059621\n",
      "Iteration 24, loss = 0.70560867\n",
      "Iteration 29, loss = 0.12893657\n",
      "Iteration 19, loss = 0.87735269\n",
      "Iteration 30, loss = 0.11953906\n",
      "Iteration 24, loss = 0.70023972\n",
      "Iteration 20, loss = 0.84420831\n",
      "Iteration 25, loss = 0.67233862\n",
      "Iteration 25, loss = 0.66804806\n",
      "Iteration 31, loss = 0.11142403\n",
      "Iteration 26, loss = 0.63812565\n",
      "Iteration 26, loss = 0.64136375\n",
      "Iteration 27, loss = 0.61086769\n",
      "Iteration 32, loss = 0.10440290\n",
      "Iteration 27, loss = 0.61287153\n",
      "Iteration 21, loss = 0.80874251\n",
      "Iteration 28, loss = 0.58583269\n",
      "Iteration 22, loss = 0.77230740\n",
      "Iteration 29, loss = 0.56281104\n",
      "Iteration 28, loss = 0.58650213\n",
      "Iteration 33, loss = 0.09832952\n",
      "Iteration 23, loss = 0.73593696\n",
      "Iteration 30, loss = 0.54172746\n",
      "Iteration 29, loss = 0.56193439\n",
      "Iteration 24, loss = 0.70049313\n",
      "Iteration 34, loss = 0.09305717\n",
      "Iteration 30, loss = 0.53929876\n",
      "Iteration 31, loss = 0.52229545\n",
      "Iteration 35, loss = 0.08846475\n",
      "Iteration 32, loss = 0.50419457\n",
      "Iteration 31, loss = 0.51838019\n",
      "Iteration 25, loss = 0.66714342\n",
      "Iteration 33, loss = 0.48717461\n",
      "Iteration 32, loss = 0.49870089\n",
      "Iteration 36, loss = 0.08445058\n",
      "Iteration 34, loss = 0.47102437\n",
      "Iteration 33, loss = 0.48006002\n",
      "Iteration 26, loss = 0.63612538\n",
      "Iteration 37, loss = 0.08092425\n",
      "Iteration 35, loss = 0.45562857\n",
      "Iteration 36, loss = 0.44083873\n",
      "Iteration 34, loss = 0.46227195\n",
      "Iteration 38, loss = 0.07781188\n",
      "Iteration 27, loss = 0.60758131\n",
      "Iteration 39, loss = 0.07505461\n",
      "Iteration 37, loss = 0.42648980\n",
      "Iteration 28, loss = 0.58107534\n",
      "Iteration 35, loss = 0.44517931\n",
      "Iteration 38, loss = 0.41251959\n",
      "Iteration 40, loss = 0.07260387\n",
      "Iteration 29, loss = 0.55645557\n",
      "Iteration 36, loss = 0.42871127\n",
      "Iteration 41, loss = 0.07041596\n",
      "Iteration 39, loss = 0.39885358\n",
      "Iteration 37, loss = 0.41270799\n",
      "Iteration 42, loss = 0.06845490\n",
      "Iteration 30, loss = 0.53407447\n",
      "Iteration 38, loss = 0.39711976\n",
      "Iteration 40, loss = 0.38542842\n",
      "Iteration 31, loss = 0.51319437\n",
      "Iteration 39, loss = 0.38190303\n",
      "Iteration 43, loss = 0.06669161\n",
      "Iteration 41, loss = 0.37218078\n",
      "Iteration 32, loss = 0.49354098\n",
      "Iteration 44, loss = 0.06510379\n",
      "Iteration 40, loss = 0.36698949\n",
      "Iteration 33, loss = 0.47490455\n",
      "Iteration 42, loss = 0.35915811\n",
      "Iteration 34, loss = 0.45710841\n",
      "Iteration 43, loss = 0.34632965\n",
      "Iteration 45, loss = 0.06367562\n",
      "Iteration 44, loss = 0.33370960\n",
      "Iteration 35, loss = 0.43996505\n",
      "Iteration 41, loss = 0.35236992\n",
      "Iteration 45, loss = 0.32128615\n",
      "Iteration 36, loss = 0.42350481\n",
      "Iteration 42, loss = 0.33805526\n",
      "Iteration 46, loss = 0.30907396\n",
      "Iteration 46, loss = 0.06237907\n",
      "Iteration 47, loss = 0.29721368\n",
      "Iteration 43, loss = 0.32409177\n",
      "Iteration 37, loss = 0.40748257\n",
      "Iteration 48, loss = 0.28564104\n",
      "Iteration 44, loss = 0.31044918\n",
      "Iteration 47, loss = 0.06121192\n",
      "Iteration 45, loss = 0.29715311\n",
      "Iteration 38, loss = 0.39190362\n",
      "Iteration 49, loss = 0.27441073\n",
      "Iteration 46, loss = 0.28437923\n",
      "Iteration 48, loss = 0.06015735\n",
      "Iteration 39, loss = 0.37672499\n",
      "Iteration 50, loss = 0.26357500\n",
      "Iteration 40, loss = 0.36187672\n",
      "Iteration 47, loss = 0.27205927\n",
      "Iteration 49, loss = 0.05920150\n",
      "Iteration 41, loss = 0.34732838\n",
      "Iteration 50, loss = 0.05833537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.33304663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.26020901\n",
      "Iteration 43, loss = 0.31914031\n",
      "Iteration 1, loss = 1.16973397\n",
      "Iteration 49, loss = 0.24889547\n",
      "Iteration 44, loss = 0.30556393\n",
      "Iteration 50, loss = 0.23816923\n",
      "Iteration 45, loss = 0.29242664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.27972097\n",
      "Iteration 47, loss = 0.26746920\n",
      "Iteration 1, loss = 1.24352638\n",
      "Iteration 48, loss = 0.25567409\n",
      "Iteration 2, loss = 1.15637455\n",
      "Iteration 1, loss = 1.16981254\n",
      "Iteration 2, loss = 1.16253890\n",
      "Iteration 49, loss = 0.24441697\n",
      "Iteration 2, loss = 1.15647112\n",
      "Iteration 3, loss = 1.09896785\n",
      "Iteration 50, loss = 0.23370473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.13956205\n",
      "Iteration 4, loss = 1.06185656\n",
      "Iteration 3, loss = 1.13968927\n",
      "Iteration 4, loss = 1.12130603\n",
      "Iteration 4, loss = 1.12148037\n",
      "Iteration 5, loss = 1.04130070\n",
      "Iteration 5, loss = 1.10299153\n",
      "Iteration 1, loss = 1.17108261\n",
      "Iteration 5, loss = 1.10323873\n",
      "Iteration 6, loss = 1.02366977\n",
      "Iteration 6, loss = 1.08492777\n",
      "Iteration 2, loss = 1.15768605\n",
      "Iteration 6, loss = 1.08542295\n",
      "Iteration 7, loss = 1.06786756\n",
      "Iteration 7, loss = 0.99566368\n",
      "Iteration 3, loss = 1.14084726\n",
      "Iteration 8, loss = 1.05844661\n",
      "Iteration 7, loss = 1.06859046\n",
      "Iteration 8, loss = 0.95283949\n",
      "Iteration 4, loss = 1.12260182\n",
      "Iteration 9, loss = 1.05433097\n",
      "Iteration 8, loss = 1.05886548\n",
      "Iteration 10, loss = 1.04964142\n",
      "Iteration 5, loss = 1.10436461\n",
      "Iteration 9, loss = 1.05493617\n",
      "Iteration 11, loss = 1.04280798\n",
      "Iteration 9, loss = 0.90486821\n",
      "Iteration 6, loss = 1.08651707\n",
      "Iteration 12, loss = 1.03251131\n",
      "Iteration 10, loss = 1.05042886\n",
      "Iteration 10, loss = 0.84978416\n",
      "Iteration 7, loss = 1.06944639\n",
      "Iteration 13, loss = 1.01789045\n",
      "Iteration 11, loss = 1.04383348\n",
      "Iteration 8, loss = 1.05886412\n",
      "Iteration 14, loss = 0.99996661\n",
      "Iteration 11, loss = 0.78671398\n",
      "Iteration 12, loss = 1.03383011\n",
      "Iteration 15, loss = 0.98035168\n",
      "Iteration 9, loss = 1.05489524\n",
      "Iteration 10, loss = 1.05054149\n",
      "Iteration 12, loss = 0.71606485\n",
      "Iteration 13, loss = 0.64109062\n",
      "Iteration 16, loss = 0.95811457\n",
      "Iteration 13, loss = 1.01951407\n",
      "Iteration 11, loss = 1.04393567\n",
      "Iteration 17, loss = 0.93202088\n",
      "Iteration 14, loss = 1.00203370\n",
      "Iteration 18, loss = 0.90349876\n",
      "Iteration 12, loss = 1.03378735\n",
      "Iteration 14, loss = 0.56705715\n",
      "Iteration 15, loss = 0.98296125\n",
      "Iteration 19, loss = 0.87327198\n",
      "Iteration 13, loss = 1.01947287\n",
      "Iteration 15, loss = 0.50010290\n",
      "Iteration 20, loss = 0.83991858\n",
      "Iteration 14, loss = 1.00175295\n",
      "Iteration 16, loss = 0.96097433\n",
      "Iteration 15, loss = 0.98241593\n",
      "Iteration 16, loss = 0.44319335\n",
      "Iteration 21, loss = 0.80436777\n",
      "Iteration 17, loss = 0.93527933\n",
      "Iteration 16, loss = 0.96013140\n",
      "Iteration 17, loss = 0.39662146\n",
      "Iteration 17, loss = 0.93423106\n",
      "Iteration 22, loss = 0.76823447\n",
      "Iteration 18, loss = 0.90661670\n",
      "Iteration 18, loss = 0.35876461\n",
      "Iteration 19, loss = 0.87625344\n",
      "Iteration 18, loss = 0.90716916\n",
      "Iteration 23, loss = 0.73264994\n",
      "Iteration 20, loss = 0.84259714\n",
      "Iteration 19, loss = 0.87737122\n",
      "Iteration 24, loss = 0.69829807\n",
      "Iteration 19, loss = 0.32683408\n",
      "Iteration 21, loss = 0.80656258\n",
      "Iteration 20, loss = 0.84481368\n",
      "Iteration 25, loss = 0.66598091\n",
      "Iteration 22, loss = 0.76951760\n",
      "Iteration 21, loss = 0.80986929\n",
      "Iteration 26, loss = 0.63609880\n",
      "Iteration 23, loss = 0.73281209\n",
      "Iteration 20, loss = 0.29852315\n",
      "Iteration 27, loss = 0.60849555\n",
      "Iteration 24, loss = 0.69742241\n",
      "Iteration 25, loss = 0.66391415\n",
      "Iteration 21, loss = 0.27201645\n",
      "Iteration 26, loss = 0.63253678\n",
      "Iteration 28, loss = 0.58319513\n",
      "Iteration 22, loss = 0.77395746\n",
      "Iteration 22, loss = 0.24659967\n",
      "Iteration 29, loss = 0.55987605\n",
      "Iteration 23, loss = 0.73823553\n",
      "Iteration 23, loss = 0.22228202\n",
      "Iteration 24, loss = 0.70377359\n",
      "Iteration 27, loss = 0.60360858\n",
      "Iteration 24, loss = 0.19954894\n",
      "Iteration 30, loss = 0.53826793\n",
      "Iteration 25, loss = 0.67127461\n",
      "Iteration 31, loss = 0.51839316\n",
      "Iteration 25, loss = 0.17903871\n",
      "Iteration 28, loss = 0.57675312\n",
      "Iteration 26, loss = 0.64116045\n",
      "Iteration 32, loss = 0.49975477\n",
      "Iteration 29, loss = 0.55182973\n",
      "Iteration 27, loss = 0.61345255\n",
      "Iteration 28, loss = 0.58798112\n",
      "Iteration 26, loss = 0.16143262\n",
      "Iteration 33, loss = 0.48208302\n",
      "Iteration 30, loss = 0.52886462\n",
      "Iteration 29, loss = 0.56450912\n",
      "Iteration 27, loss = 0.14635066\n",
      "Iteration 31, loss = 0.50743936\n",
      "Iteration 34, loss = 0.46532417\n",
      "Iteration 30, loss = 0.54280016\n",
      "Iteration 28, loss = 0.13335544\n",
      "Iteration 31, loss = 0.52285884\n",
      "Iteration 35, loss = 0.44930785\n",
      "Iteration 29, loss = 0.12211546\n",
      "Iteration 32, loss = 0.50421317\n",
      "Iteration 36, loss = 0.43387162\n",
      "Iteration 32, loss = 0.48713511\n",
      "Iteration 30, loss = 0.11240132\n",
      "Iteration 33, loss = 0.48654579\n",
      "Iteration 37, loss = 0.41888952\n",
      "Iteration 31, loss = 0.10406464\n",
      "Iteration 34, loss = 0.46982330\n",
      "Iteration 33, loss = 0.46779224\n",
      "Iteration 32, loss = 0.09689965\n",
      "Iteration 38, loss = 0.40426777\n",
      "Iteration 35, loss = 0.45382083\n",
      "Iteration 33, loss = 0.09074274\n",
      "Iteration 39, loss = 0.38997067\n",
      "Iteration 36, loss = 0.43840850\n",
      "Iteration 34, loss = 0.44937257\n",
      "Iteration 34, loss = 0.08544341\n",
      "Iteration 40, loss = 0.37595811\n",
      "Iteration 37, loss = 0.42343610\n",
      "Iteration 35, loss = 0.43164781\n",
      "Iteration 41, loss = 0.36217807\n",
      "Iteration 38, loss = 0.40882229\n",
      "Iteration 36, loss = 0.41449682\n",
      "Iteration 42, loss = 0.34860072\n",
      "Iteration 35, loss = 0.08086843\n",
      "Iteration 39, loss = 0.39448527\n",
      "Iteration 37, loss = 0.39781355\n",
      "Iteration 40, loss = 0.38035288\n",
      "Iteration 36, loss = 0.07690744\n",
      "Iteration 41, loss = 0.36644750\n",
      "Iteration 38, loss = 0.38152777\n",
      "Iteration 43, loss = 0.33522601\n",
      "Iteration 42, loss = 0.35271915\n",
      "Iteration 39, loss = 0.36559072\n",
      "Iteration 44, loss = 0.32214766\n",
      "Iteration 37, loss = 0.07346591\n",
      "Iteration 43, loss = 0.33921331\n",
      "Iteration 45, loss = 0.30938575\n",
      "Iteration 46, loss = 0.29695388\n",
      "Iteration 44, loss = 0.32590101\n",
      "Iteration 40, loss = 0.34997024\n",
      "Iteration 47, loss = 0.28487906\n",
      "Iteration 45, loss = 0.31280690\n",
      "Iteration 38, loss = 0.07046380\n",
      "Iteration 41, loss = 0.33470243\n",
      "Iteration 46, loss = 0.30009348\n",
      "Iteration 48, loss = 0.27322754\n",
      "Iteration 42, loss = 0.31976041\n",
      "Iteration 49, loss = 0.26201565\n",
      "Iteration 47, loss = 0.28767286\n",
      "Iteration 39, loss = 0.06783630\n",
      "Iteration 50, loss = 0.25121239\n",
      "Iteration 48, loss = 0.27561602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.30529960\n",
      "Iteration 49, loss = 0.26395037\n",
      "Iteration 40, loss = 0.06553053\n",
      "Iteration 44, loss = 0.29123244\n",
      "Iteration 50, loss = 0.25271229\n",
      "Iteration 41, loss = 0.06350128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.27763787\n",
      "Iteration 1, loss = 1.24297556\n",
      "Iteration 2, loss = 1.16138950\n",
      "Iteration 42, loss = 0.06171162\n",
      "Iteration 46, loss = 0.26455388\n",
      "Iteration 43, loss = 0.06012956\n",
      "Iteration 47, loss = 0.25203811\n",
      "Iteration 44, loss = 0.05872845\n",
      "Iteration 45, loss = 0.05748603\n",
      "Iteration 1, loss = 1.24489455\n",
      "Iteration 48, loss = 0.24012990\n",
      "Iteration 46, loss = 0.05638320\n",
      "Iteration 2, loss = 1.16260046\n",
      "Iteration 49, loss = 0.22880757\n",
      "Iteration 47, loss = 0.05540356\n",
      "Iteration 3, loss = 1.09801320\n",
      "Iteration 50, loss = 0.21805791\n",
      "Iteration 48, loss = 0.05453257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 0.05375760\n",
      "Iteration 4, loss = 1.06041410\n",
      "Iteration 3, loss = 1.09761963\n",
      "Iteration 4, loss = 1.06028162\n",
      "Iteration 50, loss = 0.05306768\n",
      "Iteration 5, loss = 1.03928398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.03926224\n",
      "Iteration 6, loss = 1.02057951\n",
      "Iteration 1, loss = 1.11964790\n",
      "Iteration 6, loss = 1.02126912\n",
      "Iteration 2, loss = 1.11428656\n",
      "Iteration 7, loss = 0.99063044\n",
      "Iteration 1, loss = 1.11931816\n",
      "Iteration 7, loss = 0.99286926\n",
      "Iteration 8, loss = 0.94659360\n",
      "Iteration 2, loss = 1.11393782\n",
      "Iteration 9, loss = 0.89826137\n",
      "Iteration 3, loss = 1.10898884\n",
      "Iteration 8, loss = 0.94930618\n",
      "Iteration 3, loss = 1.10862339\n",
      "Iteration 4, loss = 1.10498729\n",
      "Iteration 10, loss = 0.84173832\n",
      "Iteration 9, loss = 0.89977915\n",
      "Iteration 4, loss = 1.10461321\n",
      "Iteration 5, loss = 1.10258942\n",
      "Iteration 10, loss = 0.84340815\n",
      "Iteration 5, loss = 1.10221637\n",
      "Iteration 6, loss = 1.10132453\n",
      "Iteration 11, loss = 0.77880603\n",
      "Iteration 6, loss = 1.10095987\n",
      "Iteration 7, loss = 1.10038334\n",
      "Iteration 7, loss = 1.10003026\n",
      "Iteration 12, loss = 0.70638418\n",
      "Iteration 8, loss = 1.09907595\n",
      "Iteration 11, loss = 0.77735945\n",
      "Iteration 13, loss = 0.63005963\n",
      "Iteration 8, loss = 1.09873406\n",
      "Iteration 9, loss = 1.09708790\n",
      "Iteration 12, loss = 0.70563024\n",
      "Iteration 9, loss = 1.09675520\n",
      "Iteration 14, loss = 0.55647322\n",
      "Iteration 10, loss = 1.09447966\n",
      "Iteration 13, loss = 0.62999680\n",
      "Iteration 10, loss = 1.09415456\n",
      "Iteration 15, loss = 0.49098304\n",
      "Iteration 11, loss = 1.09152005\n",
      "Iteration 14, loss = 0.55696925\n",
      "Iteration 12, loss = 1.08849054\n",
      "Iteration 16, loss = 0.43491026\n",
      "Iteration 11, loss = 1.09120237\n",
      "Iteration 15, loss = 0.49266869\n",
      "Iteration 13, loss = 1.08555864\n",
      "Iteration 12, loss = 1.08818151\n",
      "Iteration 14, loss = 1.08274820\n",
      "Iteration 17, loss = 0.38860551\n",
      "Iteration 16, loss = 0.43838526\n",
      "Iteration 13, loss = 1.08526023\n",
      "Iteration 18, loss = 0.35072057\n",
      "Iteration 15, loss = 1.07998016\n",
      "Iteration 17, loss = 0.39409174\n",
      "Iteration 14, loss = 1.08246239\n",
      "Iteration 19, loss = 0.31873723\n",
      "Iteration 16, loss = 1.07713845\n",
      "Iteration 15, loss = 1.07970849\n",
      "Iteration 17, loss = 1.07412428\n",
      "Iteration 20, loss = 0.29041097\n",
      "Iteration 18, loss = 0.35849145\n",
      "Iteration 16, loss = 1.07688193\n",
      "Iteration 18, loss = 1.07088208\n",
      "Iteration 21, loss = 0.26384342\n",
      "Iteration 19, loss = 1.06739813\n",
      "Iteration 17, loss = 1.07388356\n",
      "Iteration 22, loss = 0.23840167\n",
      "Iteration 19, loss = 0.32850572\n",
      "Iteration 20, loss = 1.06368270\n",
      "Iteration 18, loss = 1.07065775\n",
      "Iteration 21, loss = 1.05974876\n",
      "Iteration 23, loss = 0.21400951\n",
      "Iteration 22, loss = 1.05559690\n",
      "Iteration 20, loss = 0.30187850\n",
      "Iteration 19, loss = 1.06719091\n",
      "Iteration 21, loss = 0.27676811\n",
      "Iteration 23, loss = 1.05121026\n",
      "Iteration 24, loss = 0.19142306\n",
      "Iteration 24, loss = 1.04655824\n",
      "Iteration 25, loss = 0.17143865\n",
      "Iteration 20, loss = 1.06349353\n",
      "Iteration 21, loss = 1.05957876\n",
      "Iteration 26, loss = 0.15438900\n",
      "Iteration 25, loss = 1.04160438\n",
      "Iteration 22, loss = 1.05544730\n",
      "Iteration 27, loss = 0.13987323\n",
      "Iteration 26, loss = 1.03631395\n",
      "Iteration 28, loss = 0.12738091\n",
      "Iteration 23, loss = 1.05108228\n",
      "Iteration 27, loss = 1.03065829\n",
      "Iteration 29, loss = 0.11657867\n",
      "Iteration 24, loss = 1.04645301\n",
      "Iteration 22, loss = 0.25228379\n",
      "Iteration 28, loss = 1.02461539\n",
      "Iteration 25, loss = 1.04152294\n",
      "Iteration 30, loss = 0.10728117\n",
      "Iteration 31, loss = 0.09929054\n",
      "Iteration 29, loss = 1.01816797\n",
      "Iteration 23, loss = 0.22861609\n",
      "Iteration 26, loss = 1.03625725\n",
      "Iteration 30, loss = 1.01130090\n",
      "Iteration 24, loss = 0.20632363\n",
      "Iteration 32, loss = 0.09240090\n",
      "Iteration 27, loss = 1.03062721\n",
      "Iteration 31, loss = 1.00399950\n",
      "Iteration 33, loss = 0.08646050\n",
      "Iteration 28, loss = 1.02461074\n",
      "Iteration 25, loss = 0.18672065\n",
      "Iteration 32, loss = 0.99624908\n",
      "Iteration 34, loss = 0.08131920\n",
      "Iteration 33, loss = 0.98803580\n",
      "Iteration 26, loss = 0.16976559\n",
      "Iteration 35, loss = 0.07685374\n",
      "Iteration 29, loss = 1.01819047\n",
      "Iteration 34, loss = 0.97934791\n",
      "Iteration 27, loss = 0.15498553\n",
      "Iteration 36, loss = 0.07295955\n",
      "Iteration 30, loss = 1.01135117\n",
      "Iteration 35, loss = 0.97017707\n",
      "Iteration 36, loss = 0.96051908\n",
      "Iteration 28, loss = 0.14207541\n",
      "Iteration 37, loss = 0.06954861\n",
      "Iteration 29, loss = 0.13078813\n",
      "Iteration 38, loss = 0.06654751\n",
      "Iteration 31, loss = 1.00407796\n",
      "Iteration 30, loss = 0.12098296\n",
      "Iteration 37, loss = 0.95037429\n",
      "Iteration 39, loss = 0.06389590\n",
      "Iteration 38, loss = 0.93974755\n",
      "Iteration 32, loss = 0.99635598\n",
      "Iteration 31, loss = 0.11250473\n",
      "Iteration 40, loss = 0.06154485\n",
      "Iteration 39, loss = 0.92864816\n",
      "Iteration 33, loss = 0.98817113\n",
      "Iteration 41, loss = 0.05945323\n",
      "Iteration 40, loss = 0.91708979\n",
      "Iteration 32, loss = 0.10519495\n",
      "Iteration 34, loss = 0.97951140\n",
      "Iteration 41, loss = 0.90509061\n",
      "Iteration 42, loss = 0.05758690\n",
      "Iteration 33, loss = 0.09886576\n",
      "Iteration 35, loss = 0.97036815\n",
      "Iteration 42, loss = 0.89267342\n",
      "Iteration 43, loss = 0.05591712\n",
      "Iteration 36, loss = 0.96073691\n",
      "Iteration 43, loss = 0.87986590\n",
      "Iteration 34, loss = 0.09339698\n",
      "Iteration 37, loss = 0.95061769\n",
      "Iteration 44, loss = 0.86670058\n",
      "Iteration 38, loss = 0.94001502\n",
      "Iteration 44, loss = 0.05441993\n",
      "Iteration 45, loss = 0.85321478\n",
      "Iteration 39, loss = 0.92893784\n",
      "Iteration 35, loss = 0.08864550\n",
      "Iteration 45, loss = 0.05307444\n",
      "Iteration 46, loss = 0.83945034\n",
      "Iteration 46, loss = 0.05186306\n",
      "Iteration 40, loss = 0.91739946\n",
      "Iteration 36, loss = 0.08451292\n",
      "Iteration 47, loss = 0.82545313\n",
      "Iteration 47, loss = 0.05077032\n",
      "Iteration 48, loss = 0.81127246\n",
      "Iteration 37, loss = 0.08089776\n",
      "Iteration 48, loss = 0.04978279\n",
      "Iteration 49, loss = 0.79696039\n",
      "Iteration 49, loss = 0.04888880\n",
      "Iteration 41, loss = 0.90541767\n",
      "Iteration 38, loss = 0.07772800\n",
      "Iteration 50, loss = 0.78257086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.89301493\n",
      "Iteration 50, loss = 0.04807841\n",
      "Iteration 39, loss = 0.07493095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.07245913\n",
      "Iteration 43, loss = 0.88021855\n",
      "Iteration 1, loss = 1.24284111\n",
      "Iteration 41, loss = 0.07026160\n",
      "Iteration 44, loss = 0.86706074\n",
      "Iteration 42, loss = 0.06830867\n",
      "Iteration 2, loss = 1.16121713\n",
      "Iteration 45, loss = 0.85357853\n",
      "Iteration 43, loss = 0.06656399\n",
      "Iteration 3, loss = 1.09694994\n",
      "Iteration 1, loss = 1.11952562\n",
      "Iteration 44, loss = 0.06501072\n",
      "Iteration 46, loss = 0.83981349\n",
      "Iteration 4, loss = 1.05926010\n",
      "Iteration 45, loss = 0.06362651\n",
      "Iteration 47, loss = 0.82581131\n",
      "Iteration 5, loss = 1.03805185\n",
      "Iteration 2, loss = 1.11412934\n",
      "Iteration 48, loss = 0.81162117\n",
      "Iteration 46, loss = 0.06240756\n",
      "Iteration 6, loss = 1.01993174\n",
      "Iteration 3, loss = 1.10880323\n",
      "Iteration 47, loss = 0.06135459\n",
      "Iteration 49, loss = 0.79729507\n",
      "Iteration 7, loss = 0.99190472\n",
      "Iteration 4, loss = 1.10479052\n",
      "Iteration 48, loss = 0.06047977\n",
      "Iteration 8, loss = 0.94848533\n",
      "Iteration 5, loss = 1.10240061\n",
      "Iteration 50, loss = 0.78288697\n",
      "Iteration 9, loss = 0.89635829\n",
      "Iteration 49, loss = 0.05986249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.05951650\n",
      "Iteration 10, loss = 0.83905616\n",
      "Iteration 11, loss = 0.77361910\n",
      "Iteration 6, loss = 1.10115728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11971728\n",
      "Iteration 12, loss = 0.70078752\n",
      "Iteration 7, loss = 1.10024281\n",
      "Iteration 13, loss = 0.62440472\n",
      "Iteration 2, loss = 1.11433495\n",
      "Iteration 8, loss = 1.09896072\n",
      "Iteration 3, loss = 1.10902028\n",
      "Iteration 14, loss = 0.55137530\n",
      "Iteration 4, loss = 1.10501250\n",
      "Iteration 9, loss = 1.09699412\n",
      "Iteration 1, loss = 1.24491255\n",
      "Iteration 10, loss = 1.09440478\n",
      "Iteration 2, loss = 1.16247834\n",
      "Iteration 15, loss = 0.48602707\n",
      "Iteration 5, loss = 1.10262042\n",
      "Iteration 11, loss = 1.09146449\n",
      "Iteration 6, loss = 1.10136999\n",
      "Iteration 12, loss = 1.08845732\n",
      "Iteration 16, loss = 0.42981321\n",
      "Iteration 3, loss = 1.09790850\n",
      "Iteration 13, loss = 1.08555201\n",
      "Iteration 7, loss = 1.10044669\n",
      "Iteration 14, loss = 1.08277225\n",
      "Iteration 17, loss = 0.38333515\n",
      "Iteration 8, loss = 1.09915658\n",
      "Iteration 4, loss = 1.06049693\n",
      "Iteration 9, loss = 1.09718371\n",
      "Iteration 15, loss = 1.08003811\n",
      "Iteration 18, loss = 0.34537014\n",
      "Iteration 5, loss = 1.03946650\n",
      "Iteration 10, loss = 1.09458941\n",
      "Iteration 16, loss = 1.07723263\n",
      "Iteration 19, loss = 0.31310954\n",
      "Iteration 11, loss = 1.09164436\n",
      "Iteration 20, loss = 0.28432490\n",
      "Iteration 6, loss = 1.02134287\n",
      "Iteration 17, loss = 1.07425661\n",
      "Iteration 12, loss = 1.08863167\n",
      "Iteration 21, loss = 0.25740351\n",
      "Iteration 18, loss = 1.07105460\n",
      "Iteration 7, loss = 0.99305492\n",
      "Iteration 19, loss = 1.06761338\n",
      "Iteration 13, loss = 1.08571970\n",
      "Iteration 22, loss = 0.23164222\n",
      "Iteration 8, loss = 0.94966205\n",
      "Iteration 23, loss = 0.20719635\n",
      "Iteration 14, loss = 1.08293231\n",
      "Iteration 20, loss = 1.06394383\n",
      "Iteration 9, loss = 0.90011267\n",
      "Iteration 24, loss = 0.18454395\n",
      "Iteration 15, loss = 1.08019007\n",
      "Iteration 25, loss = 0.16423928\n",
      "Iteration 21, loss = 1.06005944\n",
      "Iteration 10, loss = 0.84394673\n",
      "Iteration 26, loss = 0.14709360\n",
      "Iteration 22, loss = 1.05596110\n",
      "Iteration 16, loss = 1.07737654\n",
      "Iteration 17, loss = 1.07439280\n",
      "Iteration 27, loss = 0.13274035\n",
      "Iteration 23, loss = 1.05163210\n",
      "Iteration 18, loss = 1.07118349\n",
      "Iteration 24, loss = 1.04704183\n",
      "Iteration 28, loss = 0.12050574\n",
      "Iteration 19, loss = 1.06773523\n",
      "Iteration 25, loss = 1.04215381\n",
      "Iteration 29, loss = 0.11003374\n",
      "Iteration 20, loss = 1.06405873\n",
      "Iteration 30, loss = 0.10108571\n",
      "Iteration 26, loss = 1.03693336\n",
      "Iteration 31, loss = 0.09347517\n",
      "Iteration 21, loss = 1.06016732\n",
      "Iteration 32, loss = 0.08698805\n",
      "Iteration 27, loss = 1.03135185\n",
      "Iteration 22, loss = 1.05606185\n",
      "Iteration 28, loss = 1.02538732\n",
      "Iteration 23, loss = 1.05172559\n",
      "Iteration 29, loss = 1.01902248\n",
      "Iteration 11, loss = 0.77983797\n",
      "Iteration 33, loss = 0.08146174\n",
      "Iteration 24, loss = 1.04712797\n",
      "Iteration 30, loss = 1.01224212\n",
      "Iteration 12, loss = 0.70819367\n",
      "Iteration 34, loss = 0.07674300\n",
      "Iteration 13, loss = 0.63274992\n",
      "Iteration 31, loss = 1.00503135\n",
      "Iteration 35, loss = 0.07270050\n",
      "Iteration 25, loss = 1.04223249\n",
      "Iteration 26, loss = 1.03700434\n",
      "Iteration 14, loss = 0.55926446\n",
      "Iteration 36, loss = 0.06922357\n",
      "Iteration 32, loss = 0.99737522\n",
      "Iteration 33, loss = 0.98925951\n",
      "Iteration 15, loss = 0.49435661\n",
      "Iteration 37, loss = 0.06622074\n",
      "Iteration 34, loss = 0.98067208\n",
      "Iteration 16, loss = 0.43898560\n",
      "Iteration 38, loss = 0.06361565\n",
      "Iteration 35, loss = 0.97160409\n",
      "Iteration 17, loss = 0.39368208\n",
      "Iteration 36, loss = 0.96205084\n",
      "Iteration 18, loss = 0.35714290\n",
      "Iteration 39, loss = 0.06134482\n",
      "Iteration 27, loss = 1.03141474\n",
      "Iteration 37, loss = 0.95201207\n",
      "Iteration 19, loss = 0.32628595\n",
      "Iteration 28, loss = 1.02544148\n",
      "Iteration 40, loss = 0.05935657\n",
      "Iteration 20, loss = 0.29889131\n",
      "Iteration 38, loss = 0.94149198\n",
      "Iteration 29, loss = 1.01906697\n",
      "Iteration 21, loss = 0.27326956\n",
      "Iteration 41, loss = 0.05760850\n",
      "Iteration 39, loss = 0.93049912\n",
      "Iteration 40, loss = 0.91904635\n",
      "Iteration 30, loss = 1.01227573\n",
      "Iteration 42, loss = 0.05606550\n",
      "Iteration 41, loss = 0.90715095\n",
      "Iteration 22, loss = 0.24871182\n",
      "Iteration 31, loss = 1.00505255\n",
      "Iteration 43, loss = 0.05469722\n",
      "Iteration 23, loss = 0.22518377\n",
      "Iteration 42, loss = 0.89483481\n",
      "Iteration 32, loss = 0.99738216\n",
      "Iteration 44, loss = 0.05347927\n",
      "Iteration 43, loss = 0.88212458\n",
      "Iteration 24, loss = 0.20297588\n",
      "Iteration 44, loss = 0.86905175\n",
      "Iteration 45, loss = 0.05239156\n",
      "Iteration 25, loss = 0.18305647\n",
      "Iteration 45, loss = 0.85565261\n",
      "Iteration 46, loss = 0.05141722\n",
      "Iteration 33, loss = 0.98924999\n",
      "Iteration 26, loss = 0.16600825\n",
      "Iteration 27, loss = 0.15138778\n",
      "Iteration 47, loss = 0.05054189\n",
      "Iteration 34, loss = 0.98064353\n",
      "Iteration 46, loss = 0.84196793\n",
      "Iteration 35, loss = 0.97155354\n",
      "Iteration 28, loss = 0.13875152\n",
      "Iteration 48, loss = 0.04975387\n",
      "Iteration 47, loss = 0.82804254\n",
      "Iteration 36, loss = 0.96197495\n",
      "Iteration 49, loss = 0.04904255\n",
      "Iteration 29, loss = 0.12778873\n",
      "Iteration 48, loss = 0.81392481\n",
      "Iteration 30, loss = 0.11828822\n",
      "Iteration 50, loss = 0.04839918\n",
      "Iteration 37, loss = 0.95190710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 0.79966591\n",
      "Iteration 38, loss = 0.94135387\n",
      "Iteration 50, loss = 0.78531905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.93032351\n",
      "Iteration 40, loss = 0.91882867\n",
      "Iteration 31, loss = 0.11010222\n",
      "Iteration 41, loss = 0.90688651\n",
      "Iteration 1, loss = 1.24303969\n",
      "Iteration 42, loss = 0.89451885\n",
      "Iteration 1, loss = 1.11994885\n",
      "Iteration 32, loss = 0.10303868\n",
      "Iteration 43, loss = 0.88175242\n",
      "Iteration 2, loss = 1.11454641\n",
      "Iteration 44, loss = 0.86861891\n",
      "Iteration 33, loss = 0.09694420\n",
      "Iteration 2, loss = 1.16290708\n",
      "Iteration 34, loss = 0.09167334\n",
      "Iteration 3, loss = 1.10921092\n",
      "Iteration 3, loss = 1.09928767\n",
      "Iteration 4, loss = 1.10518619\n",
      "Iteration 45, loss = 0.85515489\n",
      "Iteration 35, loss = 0.08710154\n",
      "Iteration 46, loss = 0.84140156\n",
      "Iteration 5, loss = 1.10278256\n",
      "Iteration 4, loss = 1.06167140\n",
      "Iteration 47, loss = 0.82740430\n",
      "Iteration 6, loss = 1.10152474\n",
      "Iteration 48, loss = 0.81321208\n",
      "Iteration 36, loss = 0.08312077\n",
      "Iteration 7, loss = 1.10059559\n",
      "Iteration 5, loss = 1.04046735\n",
      "Iteration 37, loss = 0.07963901\n",
      "Iteration 8, loss = 1.09929878\n",
      "Iteration 9, loss = 1.09731725\n",
      "Iteration 38, loss = 0.07658070\n",
      "Iteration 10, loss = 1.09471256\n",
      "Iteration 6, loss = 1.02209641\n",
      "Iteration 11, loss = 1.09175646\n",
      "Iteration 39, loss = 0.07388316\n",
      "Iteration 7, loss = 0.99344841\n",
      "Iteration 12, loss = 1.08873317\n",
      "Iteration 8, loss = 0.94955883\n",
      "Iteration 40, loss = 0.07149470\n",
      "Iteration 13, loss = 1.08581177\n",
      "Iteration 9, loss = 0.90036953\n",
      "Iteration 41, loss = 0.06937138\n",
      "Iteration 14, loss = 1.08301632\n",
      "Iteration 10, loss = 0.84376289\n",
      "Iteration 15, loss = 1.08026718\n",
      "Iteration 11, loss = 0.77861095\n",
      "Iteration 42, loss = 0.06747790\n",
      "Iteration 16, loss = 1.07744753\n",
      "Iteration 12, loss = 0.70551789\n",
      "Iteration 43, loss = 0.06578334\n",
      "Iteration 49, loss = 0.79887680\n",
      "Iteration 17, loss = 1.07445812\n",
      "Iteration 44, loss = 0.06426259\n",
      "Iteration 13, loss = 0.62956311\n",
      "Iteration 18, loss = 1.07124335\n",
      "Iteration 50, loss = 0.78445242\n",
      "Iteration 14, loss = 0.55751484\n",
      "Iteration 45, loss = 0.06289414\n",
      "Iteration 19, loss = 1.06778983\n",
      "Iteration 15, loss = 0.49296780\n",
      "Iteration 46, loss = 0.06166068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.43702550\n",
      "Iteration 20, loss = 1.06410829\n",
      "Iteration 47, loss = 0.06054647\n",
      "Iteration 1, loss = 1.11959308\n",
      "Iteration 2, loss = 1.11423768\n",
      "Iteration 21, loss = 1.06021212\n",
      "Iteration 17, loss = 0.39054472\n",
      "Iteration 48, loss = 0.05953833\n",
      "Iteration 3, loss = 1.10894726\n",
      "Iteration 49, loss = 0.05862455\n",
      "Iteration 18, loss = 0.35250662\n",
      "Iteration 22, loss = 1.05610220\n",
      "Iteration 4, loss = 1.10495396\n",
      "Iteration 19, loss = 0.32069751\n",
      "Iteration 50, loss = 0.05779563\n",
      "Iteration 23, loss = 1.05176177\n",
      "Iteration 20, loss = 0.29245086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.10256543\n",
      "Iteration 21, loss = 0.26577379\n",
      "Iteration 24, loss = 1.04716018\n",
      "Iteration 6, loss = 1.10131133\n",
      "Iteration 25, loss = 1.04226084\n",
      "Iteration 22, loss = 0.24013542\n",
      "Iteration 7, loss = 1.10038255\n",
      "Iteration 23, loss = 0.21550253\n",
      "Iteration 26, loss = 1.03702887\n",
      "Iteration 1, loss = 1.12004415\n",
      "Iteration 8, loss = 1.09908896\n",
      "Iteration 24, loss = 0.19218968\n",
      "Iteration 27, loss = 1.03143544\n",
      "Iteration 9, loss = 1.09711548\n",
      "Iteration 28, loss = 1.02545834\n",
      "Iteration 10, loss = 1.09452196\n",
      "Iteration 25, loss = 0.17127878\n",
      "Iteration 2, loss = 1.11463114\n",
      "Iteration 29, loss = 1.01908003\n",
      "Iteration 11, loss = 1.09157676\n",
      "Iteration 26, loss = 0.15333579\n",
      "Iteration 30, loss = 1.01228502\n",
      "Iteration 3, loss = 1.10928842\n",
      "Iteration 12, loss = 1.08856114\n",
      "Iteration 27, loss = 0.13789260\n",
      "Iteration 13, loss = 1.08564269\n",
      "Iteration 28, loss = 0.12445080\n",
      "Iteration 4, loss = 1.10526358\n",
      "Iteration 31, loss = 1.00505816\n",
      "Iteration 5, loss = 1.10286752\n",
      "Iteration 32, loss = 0.99738423\n",
      "Iteration 14, loss = 1.08284541\n",
      "Iteration 29, loss = 0.11272442\n",
      "Iteration 6, loss = 1.10162278\n",
      "Iteration 33, loss = 0.98924868\n",
      "Iteration 30, loss = 0.10251840\n",
      "Iteration 7, loss = 1.10070906\n",
      "Iteration 34, loss = 0.98063906\n",
      "Iteration 15, loss = 1.08009035\n",
      "Iteration 31, loss = 0.09369411\n",
      "Iteration 35, loss = 0.97154615\n",
      "Iteration 8, loss = 1.09942752\n",
      "Iteration 32, loss = 0.08606019\n",
      "Iteration 16, loss = 1.07726146\n",
      "Iteration 36, loss = 0.96196490\n",
      "Iteration 9, loss = 1.09746010\n",
      "Iteration 17, loss = 1.07425987\n",
      "Iteration 10, loss = 1.09486877\n",
      "Iteration 33, loss = 0.07945015\n",
      "Iteration 18, loss = 1.07102989\n",
      "Iteration 37, loss = 0.95189470\n",
      "Iteration 19, loss = 1.06755767\n",
      "Iteration 38, loss = 0.94133938\n",
      "Iteration 11, loss = 1.09192630\n",
      "Iteration 12, loss = 1.08891795\n",
      "Iteration 39, loss = 0.93030716\n",
      "Iteration 20, loss = 1.06385341\n",
      "Iteration 34, loss = 0.07371629\n",
      "Iteration 40, loss = 0.91881062\n",
      "Iteration 35, loss = 0.06872398\n",
      "Iteration 13, loss = 1.08601344\n",
      "Iteration 21, loss = 1.05993001\n",
      "Iteration 22, loss = 1.05578804\n",
      "Iteration 41, loss = 0.90686678\n",
      "Iteration 14, loss = 1.08323701\n",
      "Iteration 36, loss = 0.06436073\n",
      "Iteration 23, loss = 1.05141062\n",
      "Iteration 37, loss = 0.06052955\n",
      "Iteration 15, loss = 1.08050901\n",
      "Iteration 42, loss = 0.89449728\n",
      "Iteration 38, loss = 0.05715016\n",
      "Iteration 24, loss = 1.04676705\n",
      "Iteration 43, loss = 0.88172861\n",
      "Iteration 16, loss = 1.07771254\n",
      "Iteration 39, loss = 0.05415355\n",
      "Iteration 25, loss = 1.04182070\n",
      "Iteration 44, loss = 0.86859218\n",
      "Iteration 40, loss = 0.05148456\n",
      "Iteration 45, loss = 0.85512422\n",
      "Iteration 26, loss = 1.03653665\n",
      "Iteration 17, loss = 1.07474843\n",
      "Iteration 46, loss = 0.84136553\n",
      "Iteration 27, loss = 1.03088594\n",
      "Iteration 18, loss = 1.07156135\n",
      "Iteration 41, loss = 0.04909812\n",
      "Iteration 28, loss = 1.02484625\n",
      "Iteration 42, loss = 0.04695615\n",
      "Iteration 47, loss = 0.82736110\n",
      "Iteration 19, loss = 1.06813828\n",
      "Iteration 29, loss = 1.01839995\n",
      "Iteration 48, loss = 0.81315947\n",
      "Iteration 43, loss = 0.04502659\n",
      "Iteration 20, loss = 1.06449041\n",
      "Iteration 49, loss = 0.79881210\n",
      "Iteration 30, loss = 1.01153157\n",
      "Iteration 44, loss = 0.04328291\n",
      "Iteration 21, loss = 1.06063163\n",
      "Iteration 31, loss = 1.00422604\n",
      "Iteration 50, loss = 0.78437254\n",
      "Iteration 22, loss = 1.05656322\n",
      "Iteration 45, loss = 0.04170343\n",
      "Iteration 32, loss = 0.99646831\n",
      "Iteration 23, loss = 1.05226881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 1.04771805\n",
      "Iteration 33, loss = 0.98824414\n",
      "Iteration 46, loss = 0.04027000\n",
      "Iteration 25, loss = 1.04287465\n",
      "Iteration 47, loss = 0.03896599\n",
      "Iteration 26, loss = 1.03770395\n",
      "Iteration 1, loss = 1.12001276\n",
      "Iteration 34, loss = 0.97954142\n",
      "Iteration 27, loss = 1.03217731\n",
      "Iteration 48, loss = 0.03777727\n",
      "Iteration 28, loss = 1.02627266\n",
      "Iteration 2, loss = 1.11459035\n",
      "Iteration 49, loss = 0.03669210\n",
      "Iteration 35, loss = 0.97035145\n",
      "Iteration 29, loss = 1.01997253\n",
      "Iteration 30, loss = 1.01326146\n",
      "Iteration 36, loss = 0.96066974\n",
      "Iteration 3, loss = 1.10923219\n",
      "Iteration 31, loss = 1.00612418\n",
      "Iteration 50, loss = 0.03570032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.99854529\n",
      "Iteration 37, loss = 0.95049639\n",
      "Iteration 4, loss = 1.10518544\n",
      "Iteration 33, loss = 0.99050996\n",
      "Iteration 38, loss = 0.93983614\n",
      "Iteration 39, loss = 0.92869822\n",
      "Iteration 34, loss = 0.98200531\n",
      "Iteration 5, loss = 1.10276122\n",
      "Iteration 40, loss = 0.91709641\n",
      "Iteration 6, loss = 1.10148291\n",
      "Iteration 35, loss = 0.97302165\n",
      "Iteration 1, loss = 1.24387256\n",
      "Iteration 36, loss = 0.96355328\n",
      "Iteration 41, loss = 0.90504911\n",
      "Iteration 2, loss = 1.16201822\n",
      "Iteration 37, loss = 0.95359888\n",
      "Iteration 7, loss = 1.10053137\n",
      "Iteration 3, loss = 1.09741345\n",
      "Iteration 38, loss = 0.94316145\n",
      "Iteration 42, loss = 0.89257955\n",
      "Iteration 8, loss = 1.09920898\n",
      "Iteration 39, loss = 0.93224828\n",
      "Iteration 43, loss = 0.87971591\n",
      "Iteration 4, loss = 1.05974494\n",
      "Iteration 40, loss = 0.92087090\n",
      "Iteration 9, loss = 1.09719865\n",
      "Iteration 44, loss = 0.86649148\n",
      "Iteration 41, loss = 0.90904517\n",
      "Iteration 5, loss = 1.03858581\n",
      "Iteration 42, loss = 0.89679154\n",
      "Iteration 10, loss = 1.09456282\n",
      "Iteration 6, loss = 1.02029716\n",
      "Iteration 43, loss = 0.88413520\n",
      "Iteration 45, loss = 0.85294444\n",
      "Iteration 11, loss = 1.09157424\n",
      "Iteration 7, loss = 0.99177908\n",
      "Iteration 44, loss = 0.87110618\n",
      "Iteration 46, loss = 0.83911766\n",
      "Iteration 8, loss = 0.94803357\n",
      "Iteration 45, loss = 0.85773934\n",
      "Iteration 46, loss = 0.84407412\n",
      "Iteration 12, loss = 1.08851775\n",
      "Iteration 9, loss = 0.89801481\n",
      "Iteration 47, loss = 0.82505817\n",
      "Iteration 47, loss = 0.83015415\n",
      "Iteration 10, loss = 0.84143501\n",
      "Iteration 13, loss = 1.08556239\n",
      "Iteration 48, loss = 0.81081656\n",
      "Iteration 48, loss = 0.81602672\n",
      "Iteration 11, loss = 0.77670004\n",
      "Iteration 49, loss = 0.79644621\n",
      "Iteration 14, loss = 1.08273177\n",
      "Iteration 49, loss = 0.80174211\n",
      "Iteration 50, loss = 0.78200245\n",
      "Iteration 12, loss = 0.70475569\n",
      "Iteration 15, loss = 1.07994559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.62907779\n",
      "Iteration 16, loss = 1.07708644\n",
      "Iteration 50, loss = 0.78735287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.55627855\n",
      "Iteration 1, loss = 1.11965549\n",
      "Iteration 17, loss = 1.07405463\n",
      "Iteration 15, loss = 0.49140065\n",
      "Iteration 2, loss = 1.11428507\n",
      "Iteration 16, loss = 0.43574134\n",
      "Iteration 18, loss = 1.07079427\n",
      "Iteration 3, loss = 1.10897597\n",
      "Iteration 1, loss = 1.11942043\n",
      "Iteration 17, loss = 0.38988850\n",
      "Iteration 19, loss = 1.06729171\n",
      "Iteration 4, loss = 1.10496185\n",
      "Iteration 18, loss = 0.35254940\n",
      "Iteration 5, loss = 1.10255065\n",
      "Iteration 20, loss = 1.06355739\n",
      "Iteration 19, loss = 0.32102087\n",
      "Iteration 2, loss = 1.11403960\n",
      "Iteration 6, loss = 1.10127147\n",
      "Iteration 7, loss = 1.10031453\n",
      "Iteration 21, loss = 1.05960433\n",
      "Iteration 20, loss = 0.29307458\n",
      "Iteration 3, loss = 1.10872618\n",
      "Iteration 22, loss = 1.05543295\n",
      "Iteration 21, loss = 0.26673502\n",
      "Iteration 4, loss = 1.10471942\n",
      "Iteration 23, loss = 1.05102601\n",
      "Iteration 8, loss = 1.09898960\n",
      "Iteration 5, loss = 1.10232869\n",
      "Iteration 9, loss = 1.09698213\n",
      "Iteration 22, loss = 0.24156759\n",
      "Iteration 24, loss = 1.04635237\n",
      "Iteration 10, loss = 1.09435278\n",
      "Iteration 25, loss = 1.04137501\n",
      "Iteration 23, loss = 0.21759578\n",
      "Iteration 24, loss = 0.19518783\n",
      "Iteration 6, loss = 1.10108057\n",
      "Iteration 26, loss = 1.03605870\n",
      "Iteration 11, loss = 1.09137057\n",
      "Iteration 25, loss = 0.17521631\n",
      "Iteration 27, loss = 1.03037432\n",
      "Iteration 12, loss = 1.08831707\n",
      "Iteration 7, loss = 1.10016078\n",
      "Iteration 13, loss = 1.08535970\n",
      "Iteration 28, loss = 1.02429945\n",
      "Iteration 26, loss = 0.15829959\n",
      "Iteration 8, loss = 1.09887494\n",
      "Iteration 14, loss = 1.08252209\n",
      "Iteration 9, loss = 1.09690635\n",
      "Iteration 29, loss = 1.01781637\n",
      "Iteration 27, loss = 0.14386762\n",
      "Iteration 10, loss = 1.09431564\n",
      "Iteration 30, loss = 1.01090953\n",
      "Iteration 28, loss = 0.13144580\n",
      "Iteration 11, loss = 1.09137317\n",
      "Iteration 31, loss = 1.00356373\n",
      "Iteration 12, loss = 1.08836212\n",
      "Iteration 15, loss = 1.07972487\n",
      "Iteration 32, loss = 0.99576380\n",
      "Iteration 13, loss = 1.08545114\n",
      "Iteration 16, loss = 1.07685163\n",
      "Iteration 29, loss = 0.12072104\n",
      "Iteration 33, loss = 0.98749539\n",
      "Iteration 14, loss = 1.08266439\n",
      "Iteration 17, loss = 1.07380327\n",
      "Iteration 15, loss = 1.07992261\n",
      "Iteration 34, loss = 0.97874628\n",
      "Iteration 30, loss = 0.11151683\n",
      "Iteration 18, loss = 1.07052395\n",
      "Iteration 16, loss = 1.07710935\n",
      "Iteration 31, loss = 0.10365131\n",
      "Iteration 17, loss = 1.07412560\n",
      "Iteration 35, loss = 0.96950770\n",
      "Iteration 19, loss = 1.06699968\n",
      "Iteration 18, loss = 1.07091584\n",
      "Iteration 32, loss = 0.09690913\n",
      "Iteration 20, loss = 1.06324044\n",
      "Iteration 36, loss = 0.95977509\n",
      "Iteration 33, loss = 0.09114054\n",
      "Iteration 21, loss = 1.05925887\n",
      "Iteration 37, loss = 0.94954848\n",
      "Iteration 19, loss = 1.06746669\n",
      "Iteration 38, loss = 0.93883250\n",
      "Iteration 22, loss = 1.05505521\n",
      "Iteration 20, loss = 1.06378884\n",
      "Iteration 34, loss = 0.08618578\n",
      "Iteration 21, loss = 1.05989575\n",
      "Iteration 39, loss = 0.92763628\n",
      "Iteration 23, loss = 1.05061226\n",
      "Iteration 22, loss = 1.05578837\n",
      "Iteration 24, loss = 1.04589903\n",
      "Iteration 35, loss = 0.08191825\n",
      "Iteration 40, loss = 0.91597338\n",
      "Iteration 23, loss = 1.05145008\n",
      "Iteration 25, loss = 1.04087871\n",
      "Iteration 36, loss = 0.07822826\n",
      "Iteration 41, loss = 0.90386199\n",
      "Iteration 26, loss = 1.03551623\n",
      "Iteration 24, loss = 1.04685037\n",
      "Iteration 42, loss = 0.89132499\n",
      "Iteration 37, loss = 0.07502250\n",
      "Iteration 25, loss = 1.04195277\n",
      "Iteration 27, loss = 1.02978264\n",
      "Iteration 43, loss = 0.87839023\n",
      "Iteration 38, loss = 0.07222301\n",
      "Iteration 26, loss = 1.03672247\n",
      "Iteration 44, loss = 0.86509049\n",
      "Iteration 39, loss = 0.06976575\n",
      "Iteration 28, loss = 1.02365569\n",
      "Iteration 27, loss = 1.03113069\n",
      "Iteration 45, loss = 0.85146348\n",
      "Iteration 40, loss = 0.06759883\n",
      "Iteration 28, loss = 1.02515521\n",
      "Iteration 29, loss = 1.01711787\n",
      "Iteration 41, loss = 0.06567904\n",
      "Iteration 46, loss = 0.83755146\n",
      "Iteration 29, loss = 1.01877848\n",
      "Iteration 30, loss = 1.01015392\n",
      "Iteration 47, loss = 0.82340081\n",
      "Iteration 42, loss = 0.06397089\n",
      "Iteration 30, loss = 1.01198501\n",
      "Iteration 31, loss = 1.00274906\n",
      "Iteration 43, loss = 0.06244499\n",
      "Iteration 32, loss = 0.99488860\n",
      "Iteration 48, loss = 0.80906143\n",
      "Iteration 44, loss = 0.06107714\n",
      "Iteration 31, loss = 1.00475961\n",
      "Iteration 49, loss = 0.79458598\n",
      "Iteration 33, loss = 0.98655877\n",
      "Iteration 50, loss = 0.78002906\n",
      "Iteration 45, loss = 0.05984665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.99708702\n",
      "Iteration 35, loss = 0.97125193\n",
      "Iteration 46, loss = 0.05873649\n",
      "Iteration 33, loss = 0.98895268\n",
      "Iteration 1, loss = 1.13211054\n",
      "Iteration 34, loss = 0.97774802\n",
      "Iteration 47, loss = 0.05773242\n",
      "Iteration 34, loss = 0.98034406\n",
      "Iteration 35, loss = 0.96844829\n",
      "Iteration 2, loss = 1.12189933\n",
      "Iteration 48, loss = 0.05682243\n",
      "Iteration 36, loss = 0.95865583\n",
      "Iteration 3, loss = 1.11180235\n",
      "Iteration 49, loss = 0.05599572\n",
      "Iteration 37, loss = 0.94837153\n",
      "Iteration 36, loss = 0.96167119\n",
      "Iteration 38, loss = 0.93760095\n",
      "Iteration 4, loss = 1.10423424\n",
      "Iteration 37, loss = 0.95160121\n",
      "Iteration 50, loss = 0.05524312\n",
      "Iteration 5, loss = 1.10008421\n",
      "Iteration 39, loss = 0.92635424\n",
      "Iteration 38, loss = 0.94104585\n",
      "Iteration 6, loss = 1.09897330\n",
      "Iteration 40, loss = 0.91464606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.93001335\n",
      "Iteration 41, loss = 0.90249574\n",
      "Iteration 7, loss = 1.09982743\n",
      "Iteration 1, loss = 1.13213028\n",
      "Iteration 42, loss = 0.88992740\n",
      "Iteration 2, loss = 1.12192620\n",
      "Iteration 8, loss = 1.10144000\n",
      "Iteration 40, loss = 0.91851639\n",
      "Iteration 43, loss = 0.87697012\n",
      "Iteration 9, loss = 1.10285226\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.86365797\n",
      "Iteration 41, loss = 0.90657211\n",
      "Iteration 3, loss = 1.11183647\n",
      "Iteration 45, loss = 0.85002992\n",
      "Iteration 42, loss = 0.89420238\n",
      "Iteration 4, loss = 1.10427397\n",
      "Iteration 46, loss = 0.83612944\n",
      "Iteration 47, loss = 0.82200410\n",
      "Iteration 5, loss = 1.10012711\n",
      "Iteration 48, loss = 0.80770484\n",
      "Iteration 1, loss = 1.13219203\n",
      "Iteration 43, loss = 0.88143396\n",
      "Iteration 44, loss = 0.86829861\n",
      "Iteration 49, loss = 0.79328524\n",
      "Iteration 2, loss = 1.12198908\n",
      "Iteration 6, loss = 1.09901699\n",
      "Iteration 45, loss = 0.85483297\n",
      "Iteration 3, loss = 1.11189959\n",
      "Iteration 50, loss = 0.77880065\n",
      "Iteration 46, loss = 0.84107835\n",
      "Iteration 4, loss = 1.10433617\n",
      "Iteration 7, loss = 1.09987013\n",
      "Iteration 47, loss = 0.82708028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.81288794\n",
      "Iteration 5, loss = 1.10018763\n",
      "Iteration 8, loss = 1.10148076\n",
      "Iteration 49, loss = 0.79855341\n",
      "Iteration 6, loss = 1.09907576\n",
      "Iteration 9, loss = 1.10289087\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13227087\n",
      "Iteration 50, loss = 0.78413093\n",
      "Iteration 7, loss = 1.09992773\n",
      "Iteration 2, loss = 1.12207133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.11198560\n",
      "Iteration 8, loss = 1.10153812\n",
      "Iteration 4, loss = 1.10442544\n",
      "Iteration 9, loss = 1.10294891\n",
      "Iteration 5, loss = 1.10027908\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13209359\n",
      "Iteration 6, loss = 1.09916821\n",
      "Iteration 2, loss = 1.12188639\n",
      "Iteration 7, loss = 1.10002026\n",
      "Iteration 1, loss = 1.13219376\n",
      "Iteration 3, loss = 1.11179291\n",
      "Iteration 8, loss = 1.10163027\n",
      "Iteration 2, loss = 1.12199556\n",
      "Iteration 4, loss = 1.10422673\n",
      "Iteration 9, loss = 1.10304071\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 1.11191013\n",
      "Iteration 1, loss = 1.13220882\n",
      "Iteration 5, loss = 1.10007686\n",
      "Iteration 1, loss = 1.12339502\n",
      "Iteration 4, loss = 1.10434903\n",
      "Iteration 2, loss = 1.12200829\n",
      "Iteration 6, loss = 1.09896470\n",
      "Iteration 5, loss = 1.10020105\n",
      "Iteration 2, loss = 1.10693708\n",
      "Iteration 3, loss = 1.11192079\n",
      "Iteration 7, loss = 1.09981671\n",
      "Iteration 3, loss = 1.09957200\n",
      "Iteration 8, loss = 1.10142684\n",
      "Iteration 4, loss = 1.10435836\n",
      "Iteration 4, loss = 1.09967432\n",
      "Iteration 9, loss = 1.10283668\n",
      "Iteration 5, loss = 1.10203852\n",
      "Iteration 5, loss = 1.10020977\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 1.09908860\n",
      "Iteration 6, loss = 1.09909713\n",
      "Iteration 6, loss = 1.10311739\n",
      "Iteration 7, loss = 1.09994800\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 1.09993961\n",
      "Iteration 8, loss = 1.10154920\n",
      "Iteration 9, loss = 1.10295968\n",
      "Iteration 1, loss = 1.13217177\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 1.10155727\n",
      "Iteration 9, loss = 1.10296713\n",
      "Iteration 1, loss = 1.12338459\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 1.10693074\n",
      "Iteration 2, loss = 1.12197488\n",
      "Iteration 3, loss = 1.11189119\n",
      "Iteration 1, loss = 1.12338812\n",
      "Iteration 4, loss = 1.10433188\n",
      "Iteration 3, loss = 1.09956733\n",
      "Iteration 1, loss = 1.13218263\n",
      "Iteration 2, loss = 1.10693655\n",
      "Iteration 4, loss = 1.09966992\n",
      "Iteration 5, loss = 1.10018531\n",
      "Iteration 2, loss = 1.12198840\n",
      "Iteration 5, loss = 1.10203444\n",
      "Iteration 3, loss = 1.09957336\n",
      "Iteration 3, loss = 1.11190880\n",
      "Iteration 6, loss = 1.09907357\n",
      "Iteration 4, loss = 1.09967495\n",
      "Iteration 6, loss = 1.10311424\n",
      "Iteration 7, loss = 1.09992444\n",
      "Iteration 5, loss = 1.10203827\n",
      "Iteration 8, loss = 1.10153317\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 1.10435435\n",
      "Iteration 9, loss = 1.10294228\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 1.10021237\n",
      "Iteration 6, loss = 1.10311710\n",
      "Iteration 1, loss = 1.12336628\n",
      "Iteration 6, loss = 1.09910399\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 1.09995651\n",
      "Iteration 1, loss = 1.13215902\n",
      "Iteration 2, loss = 1.10691281\n",
      "Iteration 8, loss = 1.10156519\n",
      "Iteration 3, loss = 1.09954843\n",
      "Iteration 2, loss = 1.12195051\n",
      "Iteration 4, loss = 1.09964947\n",
      "Iteration 9, loss = 1.10297305\n",
      "Iteration 1, loss = 1.12338817\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 1.11185617\n",
      "Iteration 2, loss = 1.10693410\n",
      "Iteration 5, loss = 1.10201243\n",
      "Iteration 3, loss = 1.09956985\n",
      "Iteration 6, loss = 1.10309072\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 1.09967134\n",
      "Iteration 1, loss = 1.12336286\n",
      "Iteration 5, loss = 1.10203475\n",
      "Iteration 1, loss = 1.12337324\n",
      "Iteration 2, loss = 1.10690587\n",
      "Iteration 4, loss = 1.10428994\n",
      "Iteration 6, loss = 1.10311341\n",
      "Iteration 5, loss = 1.10014076\n",
      "Iteration 2, loss = 1.10691374\n",
      "Iteration 3, loss = 1.09954088\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 1.09902981\n",
      "Iteration 3, loss = 1.09954677\n",
      "Iteration 4, loss = 1.09964725\n",
      "Iteration 7, loss = 1.09988337\n",
      "Iteration 4, loss = 1.09964282\n",
      "Iteration 5, loss = 1.10200966\n",
      "Iteration 5, loss = 1.10200657\n",
      "Iteration 8, loss = 1.10149527\n",
      "Iteration 6, loss = 1.10308653\n",
      "Iteration 6, loss = 1.10308507\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 1.10290710\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12336383\n",
      "Iteration 2, loss = 1.10690828\n",
      "Iteration 3, loss = 1.09954358\n",
      "Iteration 4, loss = 1.09964517\n",
      "Iteration 5, loss = 1.10200857\n",
      "Iteration 6, loss = 1.10308695\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12340583\n",
      "Iteration 2, loss = 1.10695053\n",
      "Iteration 3, loss = 1.09958540\n",
      "Iteration 4, loss = 1.09968650\n",
      "Iteration 5, loss = 1.10204979\n",
      "Iteration 6, loss = 1.10312839\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12340194\n",
      "Iteration 2, loss = 1.10694199\n",
      "Iteration 3, loss = 1.09957553\n",
      "Iteration 4, loss = 1.09967697\n",
      "Iteration 5, loss = 1.10204037\n",
      "Iteration 6, loss = 1.10311823\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12272867\n",
      "Iteration 2, loss = 1.10451383\n",
      "Iteration 3, loss = 1.08399285\n",
      "Iteration 4, loss = 1.06363717\n",
      "Iteration 5, loss = 1.04358521\n",
      "Iteration 6, loss = 1.02193282\n",
      "Iteration 7, loss = 0.99796850\n",
      "Iteration 8, loss = 0.97107855\n",
      "Iteration 9, loss = 0.94040169\n",
      "Iteration 10, loss = 0.90516202\n",
      "Iteration 11, loss = 0.86520070\n",
      "Iteration 12, loss = 0.82138657\n",
      "Iteration 13, loss = 0.77816082\n",
      "Iteration 14, loss = 0.73891511\n",
      "Iteration 15, loss = 0.69877243\n",
      "Iteration 16, loss = 0.65864534\n",
      "Iteration 17, loss = 0.62023495\n",
      "Iteration 18, loss = 0.58481552\n",
      "Iteration 19, loss = 0.55299393\n",
      "Iteration 20, loss = 0.52473671\n",
      "Iteration 21, loss = 0.49960260\n",
      "Iteration 22, loss = 0.47714124\n",
      "Iteration 23, loss = 0.45696788\n",
      "Iteration 24, loss = 0.43886550\n",
      "Iteration 25, loss = 0.42258376\n",
      "Iteration 26, loss = 0.40775687\n",
      "Iteration 27, loss = 0.39399690\n",
      "Iteration 28, loss = 0.38097188\n",
      "Iteration 29, loss = 0.36850897\n",
      "Iteration 30, loss = 0.35645366\n",
      "Iteration 31, loss = 0.34477832\n",
      "Iteration 32, loss = 0.33343334\n",
      "Iteration 33, loss = 0.32233037\n",
      "Iteration 34, loss = 0.31153696\n",
      "Iteration 35, loss = 0.30104962\n",
      "Iteration 36, loss = 0.29087905\n",
      "Iteration 37, loss = 0.28097272\n",
      "Iteration 38, loss = 0.27138233\n",
      "Iteration 39, loss = 0.26212373\n",
      "Iteration 40, loss = 0.25317457\n",
      "Iteration 41, loss = 0.24454986\n",
      "Iteration 42, loss = 0.23636807\n",
      "Iteration 43, loss = 0.22878293\n",
      "Iteration 44, loss = 0.22167910\n",
      "Iteration 45, loss = 0.21492758\n",
      "Iteration 46, loss = 0.20856998\n",
      "Iteration 47, loss = 0.20260453\n",
      "Iteration 48, loss = 0.19701302\n",
      "Iteration 49, loss = 0.19175374\n",
      "Iteration 50, loss = 0.18679933\n",
      "{'activation': 'relu', 'hidden_layer_sizes': 10}\n",
      "96.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/dev/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'hidden_layer_sizes':[10,(4,6),(9,20)], 'activation':['relu','logistic']}\n",
    "X_train = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "grid_search = GridSearchCV(MLPClassifier( max_iter=50, alpha=0.00001,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1), parameters, cv=10, n_jobs=4, scoring='accuracy')\n",
    "#print(grid_search.get_params().keys())\n",
    "\n",
    "grid_search.fit(X_train,Y)\n",
    " \n",
    " \n",
    "print(grid_search.best_params_)\n",
    "print(100*grid_search.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
